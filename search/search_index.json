{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LangGraph OpenAI Serve","text":"<p>Welcome to the documentation for <code>langgraph-openai-serve</code> - a package that provides an OpenAI-compatible API for LangGraph instances.</p>"},{"location":"#overview","title":"Overview","text":"<p>LangGraph OpenAI Serve allows you to expose your LangGraph workflows and agents through an OpenAI-compatible API interface. This enables seamless integration with any client library or tool that works with the OpenAI API, providing a standardized way to interact with your custom LangGraph solutions.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Expose your LangGraph instances through an OpenAI-compatible API</li> <li>Register multiple graphs and map them to different model names</li> <li>Use with any FastAPI application</li> <li>Support for both streaming and non-streaming completions</li> <li>Docker support for easy deployment</li> </ul>"},{"location":"#table-of-contents","title":"Table Of Contents","text":"<p>The documentation follows the best practice for project documentation as described by Daniele Procida in the Di\u00e1taxis documentation framework and consists of four separate parts:</p> <ol> <li>Tutorials - Step-by-step instructions to get you started</li> <li>How-To Guides - Practical guides for specific tasks</li> <li>Reference - Technical documentation of the API</li> <li>Explanation - Conceptual explanations of the architecture</li> </ol>"},{"location":"#installation","title":"Installation","text":"<pre><code># Using uv\nuv add langgraph-openai-serve\n\n# Using pip\npip install langgraph-openai-serve\n</code></pre>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>GitHub Repository</li> <li>Getting Started</li> <li>API Reference</li> <li>Docker Deployment</li> </ul>"},{"location":"reference/","title":"API Reference","text":"<p>This page provides detailed information about the LangGraph OpenAI Serve API endpoints and schemas.</p> <p>langgraph-openai-serve package.</p>"},{"location":"reference/#langgraph_openai_serve.GraphConfig","title":"GraphConfig","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/#langgraph_openai_serve.GraphConfig.resolve_graph","title":"resolve_graph  <code>async</code>","text":"<pre><code>resolve_graph()\n</code></pre> <p>Get the graph instance, handling both direct instances and async callables.</p> Source code in <code>src/langgraph_openai_serve/graph/graph_registry.py</code> <pre><code>async def resolve_graph(self) -&gt; CompiledStateGraph:\n    \"\"\"Get the graph instance, handling both direct instances and async callables.\"\"\"\n    if inspect.iscoroutinefunction(self.graph):\n        return await self.graph()\n    return self.graph\n</code></pre>"},{"location":"reference/#langgraph_openai_serve.GraphRegistry","title":"GraphRegistry","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/#langgraph_openai_serve.GraphRegistry.get_graph","title":"get_graph","text":"<pre><code>get_graph(name)\n</code></pre> <p>Get a graph by its name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the graph to retrieve.</p> required <p>Returns:</p> Type Description <code>GraphConfig</code> <p>The graph configuration associated with the given name.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the graph name is not found in the registry.</p> Source code in <code>src/langgraph_openai_serve/graph/graph_registry.py</code> <pre><code>def get_graph(self, name: str) -&gt; GraphConfig:\n    \"\"\"Get a graph by its name.\n\n    Args:\n        name: The name of the graph to retrieve.\n\n    Returns:\n        The graph configuration associated with the given name.\n\n    Raises:\n        ValueError: If the graph name is not found in the registry.\n    \"\"\"\n    if name not in self.registry:\n        raise ValueError(f\"Graph '{name}' not found in registry.\")\n    return self.registry[name]\n</code></pre>"},{"location":"reference/#langgraph_openai_serve.GraphRegistry.get_graph_names","title":"get_graph_names","text":"<pre><code>get_graph_names()\n</code></pre> <p>Get the names of all registered graphs.</p> Source code in <code>src/langgraph_openai_serve/graph/graph_registry.py</code> <pre><code>def get_graph_names(self) -&gt; list[str]:\n    \"\"\"Get the names of all registered graphs.\"\"\"\n    return list(self.registry.keys())\n</code></pre>"},{"location":"reference/#langgraph_openai_serve.LangchainOpenaiApiServe","title":"LangchainOpenaiApiServe","text":"<pre><code>LangchainOpenaiApiServe(app=None, graphs=None, configure_cors=False)\n</code></pre> <p>Server class to connect LangGraph instances with an OpenAI-compatible API.</p> <p>This class serves as a bridge between LangGraph instances and an OpenAI-compatible API. It allows users to register their LangGraph instances and expose them through a FastAPI application.</p> <p>Attributes:</p> Name Type Description <code>app</code> <p>The FastAPI application to attach routers to.</p> <code>graphs</code> <p>A GraphRegistry instance containing the graphs to serve.</p> <p>Initialize the server with a FastAPI app (optional) and a GraphRegistry instance (optional).</p> <p>Parameters:</p> Name Type Description Default <code>app</code> <code>FastAPI | None</code> <p>The FastAPI application to attach routers to. If None, a new FastAPI app will be created.</p> <code>None</code> <code>graphs</code> <code>GraphRegistry | None</code> <p>A GraphRegistry instance containing the graphs to serve.     If None, a default simple graph will be used.</p> <code>None</code> <code>configure_cors</code> <code>bool</code> <p>Optional; Whether to configure CORS for the FastAPI application.</p> <code>False</code> Source code in <code>src/langgraph_openai_serve/openai_server.py</code> <pre><code>def __init__(\n    self,\n    app: FastAPI | None = None,\n    graphs: GraphRegistry | None = None,\n    configure_cors: bool = False,\n):\n    \"\"\"Initialize the server with a FastAPI app (optional) and a GraphRegistry instance (optional).\n\n    Args:\n        app: The FastAPI application to attach routers to. If None, a new FastAPI app will be created.\n        graphs: A GraphRegistry instance containing the graphs to serve.\n                If None, a default simple graph will be used.\n        configure_cors: Optional; Whether to configure CORS for the FastAPI application.\n    \"\"\"\n    self.app = app\n\n    if app is None:\n        app = FastAPI(\n            title=\"LangGraph OpenAI Compatible API\",\n            description=\"An OpenAI-compatible API for LangGraph\",\n            version=\"0.0.1\",\n        )\n    self.app = app\n\n    if graphs is None:\n        logger.info(\"Graphs not provided, using default simple graph\")\n        default_graph_config = GraphConfig(graph=simple_graph)\n        self.graph_registry = GraphRegistry(\n            registry={\"simple-graph\": default_graph_config}\n        )\n    elif isinstance(graphs, GraphRegistry):\n        logger.info(\"Using provided GraphRegistry instance\")\n        self.graph_registry = graphs\n    else:\n        raise TypeError(\n            \"Invalid type for graphs parameter. Expected GraphRegistry or None.\"\n        )\n\n    # Attach the registry to the app's state for dependency injection\n    self.app.state.graph_registry = self.graph_registry\n\n    # Configure CORS if requested\n    if configure_cors:\n        self._configure_cors()\n\n    logger.info(\n        f\"Initialized LangchainOpenaiApiServe with {len(self.graph_registry.registry)} graphs\"\n    )\n    logger.info(\n        f\"Available graphs: {', '.join(self.graph_registry.get_graph_names())}\"\n    )\n</code></pre>"},{"location":"reference/#langgraph_openai_serve.LangchainOpenaiApiServe.bind_openai_chat_completion","title":"bind_openai_chat_completion","text":"<pre><code>bind_openai_chat_completion(prefix='/v1')\n</code></pre> <p>Bind OpenAI-compatible chat completion endpoints to the FastAPI app.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>Optional; The URL prefix for the OpenAI-compatible endpoints. Defaults to \"/v1\".</p> <code>'/v1'</code> Source code in <code>src/langgraph_openai_serve/openai_server.py</code> <pre><code>def bind_openai_chat_completion(self, prefix: str = \"/v1\"):\n    \"\"\"Bind OpenAI-compatible chat completion endpoints to the FastAPI app.\n\n    Args:\n        prefix: Optional; The URL prefix for the OpenAI-compatible endpoints. Defaults to \"/v1\".\n    \"\"\"\n    self.app.include_router(chat_views.router, prefix=prefix)\n    self.app.include_router(health_views.router, prefix=prefix)\n    self.app.include_router(models_views.router, prefix=prefix)\n\n    logger.info(f\"Bound OpenAI chat completion endpoints with prefix: {prefix}\")\n\n    return self\n</code></pre>"},{"location":"reference/#langgraph_openai_serve.api","title":"api","text":""},{"location":"reference/#langgraph_openai_serve.api.chat","title":"chat","text":""},{"location":"reference/#langgraph_openai_serve.api.chat.schemas","title":"schemas","text":"<p>Pydantic models for the OpenAI API.</p> <p>This module defines Pydantic models that match the OpenAI API request and response formats.</p>"},{"location":"reference/#langgraph_openai_serve.api.chat.schemas.ChatCompletionRequest","title":"ChatCompletionRequest","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for a chat completion request.</p>"},{"location":"reference/#langgraph_openai_serve.api.chat.schemas.ChatCompletionRequestMessage","title":"ChatCompletionRequestMessage","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for a chat completion request message.</p>"},{"location":"reference/#langgraph_openai_serve.api.chat.schemas.ChatCompletionResponse","title":"ChatCompletionResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for a chat completion response.</p>"},{"location":"reference/#langgraph_openai_serve.api.chat.schemas.ChatCompletionResponseChoice","title":"ChatCompletionResponseChoice","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for a chat completion response choice.</p>"},{"location":"reference/#langgraph_openai_serve.api.chat.schemas.ChatCompletionResponseMessage","title":"ChatCompletionResponseMessage","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for a chat completion response message.</p>"},{"location":"reference/#langgraph_openai_serve.api.chat.schemas.ChatCompletionStreamResponse","title":"ChatCompletionStreamResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for a chat completion stream response.</p>"},{"location":"reference/#langgraph_openai_serve.api.chat.schemas.ChatCompletionStreamResponseChoice","title":"ChatCompletionStreamResponseChoice","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for a chat completion stream response choice.</p>"},{"location":"reference/#langgraph_openai_serve.api.chat.schemas.ChatCompletionStreamResponseDelta","title":"ChatCompletionStreamResponseDelta","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for a chat completion stream response delta.</p>"},{"location":"reference/#langgraph_openai_serve.api.chat.schemas.ChatMessage","title":"ChatMessage","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for a chat message.</p>"},{"location":"reference/#langgraph_openai_serve.api.chat.schemas.FunctionCall","title":"FunctionCall","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for a function call.</p>"},{"location":"reference/#langgraph_openai_serve.api.chat.schemas.FunctionDefinition","title":"FunctionDefinition","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for a function definition.</p>"},{"location":"reference/#langgraph_openai_serve.api.chat.schemas.Role","title":"Role","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Role options for chat messages.</p>"},{"location":"reference/#langgraph_openai_serve.api.chat.schemas.Tool","title":"Tool","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for a tool.</p>"},{"location":"reference/#langgraph_openai_serve.api.chat.schemas.ToolCall","title":"ToolCall","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for a tool call.</p>"},{"location":"reference/#langgraph_openai_serve.api.chat.schemas.ToolCallFunction","title":"ToolCallFunction","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for a tool call function.</p>"},{"location":"reference/#langgraph_openai_serve.api.chat.schemas.ToolFunction","title":"ToolFunction","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for a tool function.</p>"},{"location":"reference/#langgraph_openai_serve.api.chat.schemas.UsageInfo","title":"UsageInfo","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for usage information.</p>"},{"location":"reference/#langgraph_openai_serve.api.chat.service","title":"service","text":"<p>Chat completion service.</p> <p>This module provides a service for handling chat completions, implementing business logic that was previously in the router.</p>"},{"location":"reference/#langgraph_openai_serve.api.chat.service.ChatCompletionService","title":"ChatCompletionService","text":"<p>Service for handling chat completions.</p>"},{"location":"reference/#langgraph_openai_serve.api.chat.service.ChatCompletionService.generate_completion","title":"generate_completion  <code>async</code>","text":"<pre><code>generate_completion(chat_request, graph_registry)\n</code></pre> <p>Generate a chat completion.</p> <p>Parameters:</p> Name Type Description Default <code>chat_request</code> <code>ChatCompletionRequest</code> <p>The chat completion request.</p> required <code>graph_registry</code> <code>GraphRegistry</code> <p>The GraphRegistry object containing registered graphs.</p> required <p>Returns:</p> Type Description <code>ChatCompletionResponse</code> <p>A chat completion response.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there is an error generating the completion.</p> Source code in <code>src/langgraph_openai_serve/api/chat/service.py</code> <pre><code>async def generate_completion(\n    self, chat_request: ChatCompletionRequest, graph_registry: GraphRegistry\n) -&gt; ChatCompletionResponse:\n    \"\"\"Generate a chat completion.\n\n    Args:\n        chat_request: The chat completion request.\n        graph_registry: The GraphRegistry object containing registered graphs.\n\n    Returns:\n        A chat completion response.\n\n    Raises:\n        Exception: If there is an error generating the completion.\n    \"\"\"\n    start_time = time.time()\n\n    # Get the completion from the LangGraph model\n    completion, tokens_used = await run_langgraph(\n        model=chat_request.model,\n        messages=chat_request.messages,\n        graph_registry=graph_registry,\n    )\n\n    # Build the response\n    response = ChatCompletionResponse(\n        id=f\"chatcmpl-{uuid.uuid4()}\",\n        created=int(time.time()),\n        model=chat_request.model,\n        choices=[\n            ChatCompletionResponseChoice(\n                index=0,\n                message=ChatCompletionResponseMessage(\n                    role=Role.ASSISTANT,\n                    content=completion,\n                ),\n                finish_reason=\"stop\",\n            )\n        ],\n        usage=UsageInfo(\n            prompt_tokens=tokens_used[\"prompt_tokens\"],\n            completion_tokens=tokens_used[\"completion_tokens\"],\n            total_tokens=tokens_used[\"total_tokens\"],\n        ),\n    )\n\n    logger.info(\n        f\"Chat completion finished in {time.time() - start_time:.2f}s. \"\n        f\"Total tokens: {tokens_used['total_tokens']}\"\n    )\n\n    return response\n</code></pre>"},{"location":"reference/#langgraph_openai_serve.api.chat.service.ChatCompletionService.stream_completion","title":"stream_completion  <code>async</code>","text":"<pre><code>stream_completion(chat_request, graph_registry)\n</code></pre> <p>Stream a chat completion response.</p> <p>Parameters:</p> Name Type Description Default <code>chat_request</code> <code>ChatCompletionRequest</code> <p>The chat completion request.</p> required <code>graph_registry</code> <code>GraphRegistry</code> <p>The GraphRegistry object containing registered graphs.</p> required <p>Yields:</p> Type Description <code>AsyncIterator[str]</code> <p>Chunks of the chat completion response.</p> Source code in <code>src/langgraph_openai_serve/api/chat/service.py</code> <pre><code>async def stream_completion(\n    self, chat_request: ChatCompletionRequest, graph_registry: GraphRegistry\n) -&gt; AsyncIterator[str]:\n    \"\"\"Stream a chat completion response.\n\n    Args:\n        chat_request: The chat completion request.\n        graph_registry: The GraphRegistry object containing registered graphs.\n\n    Yields:\n        Chunks of the chat completion response.\n    \"\"\"\n    start_time = time.time()\n    response_id = f\"chatcmpl-{uuid.uuid4()}\"\n    created = int(time.time())\n\n    try:\n        # Send the initial response with the role\n        yield self._format_stream_chunk(\n            ChatCompletionStreamResponse(\n                id=response_id,\n                created=created,\n                model=chat_request.model,\n                choices=[\n                    ChatCompletionStreamResponseChoice(\n                        index=0,\n                        delta=ChatCompletionStreamResponseDelta(\n                            role=Role.ASSISTANT,\n                        ),\n                        finish_reason=None,\n                    )\n                ],\n            )\n        )\n\n        # Stream the completion from the LangGraph model\n        async for chunk, _ in run_langgraph_stream(\n            model=chat_request.model,\n            messages=chat_request.messages,\n            graph_registry=graph_registry,\n        ):\n            # Send the content chunk\n            yield self._format_stream_chunk(\n                ChatCompletionStreamResponse(\n                    id=response_id,\n                    created=created,\n                    model=chat_request.model,\n                    choices=[\n                        ChatCompletionStreamResponseChoice(\n                            index=0,\n                            delta=ChatCompletionStreamResponseDelta(\n                                content=chunk,\n                            ),\n                            finish_reason=None,\n                        )\n                    ],\n                )\n            )\n\n        # Send the final response with finish_reason\n        yield self._format_stream_chunk(\n            ChatCompletionStreamResponse(\n                id=response_id,\n                created=created,\n                model=chat_request.model,\n                choices=[\n                    ChatCompletionStreamResponseChoice(\n                        index=0,\n                        delta=ChatCompletionStreamResponseDelta(),\n                        finish_reason=\"stop\",\n                    )\n                ],\n            )\n        )\n\n        # Send the [DONE] message\n        yield \"data: [DONE]\\n\\n\"\n\n        logger.info(\n            f\"Streamed chat completion finished in {time.time() - start_time:.2f}s\"\n        )\n\n    except Exception as e:\n        logger.exception(\"Error streaming chat completion\")\n        # In case of an error, send an error message\n        error_response = {\"error\": {\"message\": str(e), \"type\": \"server_error\"}}\n        yield f\"data: {json.dumps(error_response)}\\n\\n\"\n        yield \"data: [DONE]\\n\\n\"\n</code></pre>"},{"location":"reference/#langgraph_openai_serve.api.chat.views","title":"views","text":"<p>Chat completion router.</p> <p>This module provides the FastAPI router for the chat completion endpoint, implementing an OpenAI-compatible interface.</p>"},{"location":"reference/#langgraph_openai_serve.api.chat.views.create_chat_completion","title":"create_chat_completion  <code>async</code>","text":"<pre><code>create_chat_completion(chat_request, service, graph_registry)\n</code></pre> <p>Create a chat completion.</p> <p>This endpoint is compatible with OpenAI's chat completion API.</p> <p>Parameters:</p> Name Type Description Default <code>chat_request</code> <code>ChatCompletionRequest</code> <p>The parsed chat completion request.</p> required <code>graph_registry</code> <code>Annotated[GraphRegistry, Depends(get_graph_registry_dependency)]</code> <p>The graph registry dependency.</p> required <code>service</code> <code>Annotated[ChatCompletionService, Depends(ChatCompletionService)]</code> <p>The chat completion service dependency.</p> required <p>Returns:</p> Type Description <code>StreamingResponse | ChatCompletionResponse</code> <p>A chat completion response, either as a complete response or as a stream.</p> Source code in <code>src/langgraph_openai_serve/api/chat/views.py</code> <pre><code>@router.post(\"/chat/completions\", response_model=ChatCompletionResponse)\nasync def create_chat_completion(\n    chat_request: ChatCompletionRequest,\n    service: Annotated[ChatCompletionService, Depends(ChatCompletionService)],\n    graph_registry: Annotated[GraphRegistry, Depends(get_graph_registry_dependency)],\n) -&gt; StreamingResponse | ChatCompletionResponse:\n    \"\"\"Create a chat completion.\n\n    This endpoint is compatible with OpenAI's chat completion API.\n\n    Args:\n        chat_request: The parsed chat completion request.\n        graph_registry: The graph registry dependency.\n        service: The chat completion service dependency.\n\n    Returns:\n        A chat completion response, either as a complete response or as a stream.\n    \"\"\"\n\n    logger.info(\n        f\"Received chat completion request for model: {chat_request.model}, \"\n        f\"stream: {chat_request.stream}\"\n    )\n\n    if chat_request.stream:\n        logger.info(\"Streaming chat completion response\")\n        return StreamingResponse(\n            service.stream_completion(chat_request, graph_registry),\n            media_type=\"text/event-stream\",\n        )\n\n    logger.info(\"Generating non-streaming chat completion response\")\n    response = await service.generate_completion(chat_request, graph_registry)\n    logger.info(\"Returning non-streaming chat completion response\")\n    return response\n</code></pre>"},{"location":"reference/#langgraph_openai_serve.api.health","title":"health","text":""},{"location":"reference/#langgraph_openai_serve.api.health.views","title":"views","text":""},{"location":"reference/#langgraph_openai_serve.api.health.views.health_check","title":"health_check","text":"<pre><code>health_check()\n</code></pre> <p>Checks the health of a project.</p> <p>It returns 200 if the project is healthy.</p> Source code in <code>src/langgraph_openai_serve/api/health/views.py</code> <pre><code>@router.get(\"\")\ndef health_check() -&gt; None:\n    \"\"\"Checks the health of a project.\n\n    It returns 200 if the project is healthy.\n    \"\"\"\n</code></pre>"},{"location":"reference/#langgraph_openai_serve.api.models","title":"models","text":""},{"location":"reference/#langgraph_openai_serve.api.models.schemas","title":"schemas","text":""},{"location":"reference/#langgraph_openai_serve.api.models.schemas.Model","title":"Model","text":"<p>               Bases: <code>BaseModel</code></p> <p>Individual model information.</p>"},{"location":"reference/#langgraph_openai_serve.api.models.schemas.ModelList","title":"ModelList","text":"<p>               Bases: <code>BaseModel</code></p> <p>List of available models.</p>"},{"location":"reference/#langgraph_openai_serve.api.models.schemas.ModelPermission","title":"ModelPermission","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model permission information.</p>"},{"location":"reference/#langgraph_openai_serve.api.models.service","title":"service","text":"<p>Model service.</p> <p>This module provides a service for handling OpenAI model information.</p>"},{"location":"reference/#langgraph_openai_serve.api.models.service.ModelService","title":"ModelService","text":"<p>Service for handling model operations.</p>"},{"location":"reference/#langgraph_openai_serve.api.models.service.ModelService.get_models","title":"get_models","text":"<pre><code>get_models(graph_registry)\n</code></pre> <p>Get a list of available models.</p> <p>Parameters:</p> Name Type Description Default <code>graph_registry</code> <code>GraphRegistry</code> <p>The GraphRegistry containing registered graphs.</p> required <p>Returns:</p> Type Description <code>ModelList</code> <p>A list of models in OpenAI compatible format.</p> Source code in <code>src/langgraph_openai_serve/api/models/service.py</code> <pre><code>def get_models(self, graph_registry: GraphRegistry) -&gt; ModelList:\n    \"\"\"Get a list of available models.\n\n    Args:\n        graph_registry: The GraphRegistry containing registered graphs.\n\n    Returns:\n        A list of models in OpenAI compatible format.\n    \"\"\"\n    permission = ModelPermission(\n        id=\"modelperm-04cadfeee8ad4eb8ad479a5af3bc261d\",\n        created=1743771509,\n        allow_create_engine=False,\n        allow_sampling=True,\n        allow_logprobs=True,\n        allow_search_indices=False,\n        allow_view=True,\n        allow_fine_tuning=False,\n        organization=\"*\",\n        group=None,\n        is_blocking=False,\n    )\n\n    models = [\n        Model(\n            id=name,\n            created=1743771509,\n            owned_by=\"langgraph-openai-serve\",\n            root=f\"{name}-root\",\n            parent=None,\n            max_model_len=16000,\n            permission=[permission],\n        )\n        for name in graph_registry.registry\n    ]\n\n    logger.info(f\"Retrieved {len(models)} available models\")\n    return ModelList(data=models)\n</code></pre>"},{"location":"reference/#langgraph_openai_serve.api.models.views","title":"views","text":"<p>Models router.</p> <p>This module provides the FastAPI router for the models endpoint, implementing an OpenAI-compatible interface for model listing.</p>"},{"location":"reference/#langgraph_openai_serve.api.models.views.get_graph_registry_dependency","title":"get_graph_registry_dependency","text":"<pre><code>get_graph_registry_dependency(request)\n</code></pre> <p>Dependency to get the graph registry from the app state.</p> Source code in <code>src/langgraph_openai_serve/api/models/views.py</code> <pre><code>def get_graph_registry_dependency(request: Request) -&gt; GraphRegistry:\n    \"\"\"Dependency to get the graph registry from the app state.\"\"\"\n    return request.app.state.graph_registry\n</code></pre>"},{"location":"reference/#langgraph_openai_serve.api.models.views.list_models","title":"list_models","text":"<pre><code>list_models(service, graph_registry)\n</code></pre> <p>Get a list of available models.</p> Source code in <code>src/langgraph_openai_serve/api/models/views.py</code> <pre><code>@router.get(\"\", response_model=ModelList)\ndef list_models(\n    service: Annotated[ModelService, Depends(ModelService)],\n    graph_registry: Annotated[GraphRegistry, Depends(get_graph_registry_dependency)],\n):\n    \"\"\"Get a list of available models.\"\"\"\n    logger.info(\"Received request to list models\")\n    models = service.get_models(graph_registry)\n    logger.info(f\"Returning {len(models.data)} models\")\n    return models\n</code></pre>"},{"location":"reference/#langgraph_openai_serve.core","title":"core","text":""},{"location":"reference/#langgraph_openai_serve.core.settings","title":"settings","text":""},{"location":"reference/#langgraph_openai_serve.core.settings.Settings","title":"Settings","text":"<p>               Bases: <code>BaseSettings</code></p> <p>This class is used to load environment variables either from environment or from a .env file and store them as class attributes. NOTE:     - environment variables will always take priority over values loaded from a dotenv file     - environment variable names are case-insensitive     - environment variable type is inferred from the type hint of the class attribute     - For environment variables that are not set, a default value should be provided</p> <p>For more info, see the related pydantic docs: https://docs.pydantic.dev/latest/concepts/pydantic_settings</p>"},{"location":"reference/#langgraph_openai_serve.core.settings.Settings.check_langfuse_settings","title":"check_langfuse_settings","text":"<pre><code>check_langfuse_settings(v)\n</code></pre> <p>Validate Langfuse settings if enabled.</p> Source code in <code>src/langgraph_openai_serve/core/settings.py</code> <pre><code>@field_validator(\"ENABLE_LANGFUSE\")\ndef check_langfuse_settings(cls, v: bool) -&gt; bool:\n    \"\"\"Validate Langfuse settings if enabled.\"\"\"\n    if v is False:\n        return v\n\n    # Check if langfuse package is installed\n    if importlib.util.find_spec(\"langfuse\") is None:\n        raise RuntimeError(\n            \"Langfuse is enabled but the 'langfuse' package is not installed. \"\n            \"Please install it, e.g., with `uv add langgraph-openai-serve[tracing]`.\"\n        )\n\n    # Check for required environment variables\n    required_env_vars = [\n        \"LANGFUSE_HOST\",\n        \"LANGFUSE_PUBLIC_KEY\",\n        \"LANGFUSE_SECRET_KEY\",\n    ]\n    missing_vars = [var for var in required_env_vars if os.getenv(var) is None]\n\n    if missing_vars:\n        raise RuntimeError(\n            \"Langfuse is enabled but the following environment variables are not set: \"\n            f\"{', '.join(missing_vars)}. Please set these variables.\"\n        )\n\n    return v\n</code></pre>"},{"location":"reference/#langgraph_openai_serve.graph","title":"graph","text":"<p>Service package for the LangGraph OpenAI compatible API.</p>"},{"location":"reference/#langgraph_openai_serve.graph.graph_registry","title":"graph_registry","text":""},{"location":"reference/#langgraph_openai_serve.graph.graph_registry.GraphConfig","title":"GraphConfig","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/#langgraph_openai_serve.graph.graph_registry.GraphConfig.resolve_graph","title":"resolve_graph  <code>async</code>","text":"<pre><code>resolve_graph()\n</code></pre> <p>Get the graph instance, handling both direct instances and async callables.</p> Source code in <code>src/langgraph_openai_serve/graph/graph_registry.py</code> <pre><code>async def resolve_graph(self) -&gt; CompiledStateGraph:\n    \"\"\"Get the graph instance, handling both direct instances and async callables.\"\"\"\n    if inspect.iscoroutinefunction(self.graph):\n        return await self.graph()\n    return self.graph\n</code></pre>"},{"location":"reference/#langgraph_openai_serve.graph.graph_registry.GraphRegistry","title":"GraphRegistry","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/#langgraph_openai_serve.graph.graph_registry.GraphRegistry.get_graph","title":"get_graph","text":"<pre><code>get_graph(name)\n</code></pre> <p>Get a graph by its name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the graph to retrieve.</p> required <p>Returns:</p> Type Description <code>GraphConfig</code> <p>The graph configuration associated with the given name.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the graph name is not found in the registry.</p> Source code in <code>src/langgraph_openai_serve/graph/graph_registry.py</code> <pre><code>def get_graph(self, name: str) -&gt; GraphConfig:\n    \"\"\"Get a graph by its name.\n\n    Args:\n        name: The name of the graph to retrieve.\n\n    Returns:\n        The graph configuration associated with the given name.\n\n    Raises:\n        ValueError: If the graph name is not found in the registry.\n    \"\"\"\n    if name not in self.registry:\n        raise ValueError(f\"Graph '{name}' not found in registry.\")\n    return self.registry[name]\n</code></pre>"},{"location":"reference/#langgraph_openai_serve.graph.graph_registry.GraphRegistry.get_graph_names","title":"get_graph_names","text":"<pre><code>get_graph_names()\n</code></pre> <p>Get the names of all registered graphs.</p> Source code in <code>src/langgraph_openai_serve/graph/graph_registry.py</code> <pre><code>def get_graph_names(self) -&gt; list[str]:\n    \"\"\"Get the names of all registered graphs.\"\"\"\n    return list(self.registry.keys())\n</code></pre>"},{"location":"reference/#langgraph_openai_serve.graph.runner","title":"runner","text":"<p>LangGraph runner service.</p> <p>This module provides functionality to run LangGraph models with an OpenAI-compatible interface. It handles conversion between OpenAI's message format and LangChain's message format, and provides both streaming and non-streaming interfaces for running LangGraph workflows.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from langgraph_openai_serve.services.graph_runner import run_langgraph\n&gt;&gt;&gt; response, usage = await run_langgraph(\"my-model\", messages, registry)\n&gt;&gt;&gt; from langgraph_openai_serve.services.graph_runner import run_langgraph_stream\n&gt;&gt;&gt; async for chunk, metrics in run_langgraph_stream(\"my-model\", messages, registry):\n...     print(chunk)\n</code></pre> <p>The module contains the following functions: - <code>convert_to_lc_messages(messages)</code> - Converts OpenAI messages to LangChain messages. - <code>register_graphs(graphs)</code> - Validates and returns the provided graph dictionary. - <code>run_langgraph(model, messages, graph_registry)</code> - Runs a LangGraph model with the given messages. - <code>run_langgraph_stream(model, messages, graph_registry)</code> - Runs a LangGraph model in streaming mode.</p>"},{"location":"reference/#langgraph_openai_serve.graph.runner.register_graphs","title":"register_graphs","text":"<pre><code>register_graphs(graphs)\n</code></pre> <p>Validate and return the provided graph dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>graphs</code> <code>Dict[str, Any]</code> <p>A dictionary mapping graph names to LangGraph instances.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The validated graph dictionary.</p> Source code in <code>src/langgraph_openai_serve/graph/runner.py</code> <pre><code>def register_graphs(graphs: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Validate and return the provided graph dictionary.\n\n    Args:\n        graphs: A dictionary mapping graph names to LangGraph instances.\n\n    Returns:\n        The validated graph dictionary.\n    \"\"\"\n    # Potential future validation can go here\n    logger.info(f\"Registered {len(graphs)} graphs: {', '.join(graphs.keys())}\")\n    return graphs\n</code></pre>"},{"location":"reference/#langgraph_openai_serve.graph.runner.run_langgraph","title":"run_langgraph  <code>async</code>","text":"<pre><code>run_langgraph(model, messages, graph_registry)\n</code></pre> <p>Run a LangGraph model with the given messages using the compiled workflow.</p> <p>This function processes input messages through a LangGraph workflow and returns the generated response along with token usage information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; response, usage = await run_langgraph(\"my-model\", messages, registry)\n&gt;&gt;&gt; print(response)\n&gt;&gt;&gt; print(usage)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The name of the model to use, which also determines which graph to use.</p> required <code>messages</code> <code>list[ChatCompletionRequestMessage]</code> <p>A list of messages to process through the LangGraph.</p> required <code>graph_registry</code> <code>GraphRegistry</code> <p>The GraphRegistry instance containing registered graphs.</p> required <p>Returns:</p> Type Description <code>tuple[str, dict[str, int]]</code> <p>A tuple containing the generated response string and a dictionary of token usage information.</p> Source code in <code>src/langgraph_openai_serve/graph/runner.py</code> <pre><code>async def run_langgraph(\n    model: str,\n    messages: list[ChatCompletionRequestMessage],\n    graph_registry: GraphRegistry,\n) -&gt; tuple[str, dict[str, int]]:\n    \"\"\"Run a LangGraph model with the given messages using the compiled workflow.\n\n    This function processes input messages through a LangGraph workflow and returns\n    the generated response along with token usage information.\n\n    Examples:\n        &gt;&gt;&gt; response, usage = await run_langgraph(\"my-model\", messages, registry)\n        &gt;&gt;&gt; print(response)\n        &gt;&gt;&gt; print(usage)\n\n    Args:\n        model: The name of the model to use, which also determines which graph to use.\n        messages: A list of messages to process through the LangGraph.\n        graph_registry: The GraphRegistry instance containing registered graphs.\n\n    Returns:\n        A tuple containing the generated response string and a dictionary of token usage information.\n    \"\"\"\n    logger.info(f\"Running LangGraph model {model} with {len(messages)} messages\")\n    start_time = time.time()\n\n    # Use graph_registry.get_graph to get the graph config and then the graph\n    try:\n        graph_config = graph_registry.get_graph(model)\n        graph = await graph_config.resolve_graph()\n    except ValueError as e:\n        logger.error(f\"Error getting graph for model '{model}': {e}\")\n        raise e\n\n    # Convert OpenAI messages to LangChain messages\n    lc_messages = convert_to_lc_messages(messages)\n\n    # Run the graph with the messages\n    result = await graph.ainvoke({\"messages\": lc_messages})\n    response = result[\"messages\"][-1].content if result[\"messages\"] else \"\"\n\n    # Calculate token usage (approximate)\n    prompt_tokens = sum(len((m.content or \"\").split()) for m in messages)\n    completion_tokens = len((response or \"\").split())\n    token_usage = {\n        \"prompt_tokens\": prompt_tokens,\n        \"completion_tokens\": completion_tokens,\n        \"total_tokens\": prompt_tokens + completion_tokens,\n    }\n\n    logger.info(f\"LangGraph completion generated in {time.time() - start_time:.2f}s\")\n    return response, token_usage\n</code></pre>"},{"location":"reference/#langgraph_openai_serve.graph.runner.run_langgraph_stream","title":"run_langgraph_stream  <code>async</code>","text":"<pre><code>run_langgraph_stream(model, messages, graph_registry)\n</code></pre> <p>Run a LangGraph model in streaming mode.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The name of the model (graph) to run.</p> required <code>messages</code> <code>list[ChatCompletionRequestMessage]</code> <p>A list of OpenAI-compatible messages.</p> required <code>graph_registry</code> <code>GraphRegistry</code> <p>The registry containing the graph configurations.</p> required <p>Yields:</p> Type Description <code>AsyncGenerator[tuple[str, dict[str, int]], None]</code> <p>A tuple containing the content chunk and token usage metrics.</p> Source code in <code>src/langgraph_openai_serve/graph/runner.py</code> <pre><code>async def run_langgraph_stream(\n    model: str,\n    messages: list[ChatCompletionRequestMessage],\n    graph_registry: GraphRegistry,\n) -&gt; AsyncGenerator[tuple[str, dict[str, int]], None]:\n    \"\"\"Run a LangGraph model in streaming mode.\n\n    Args:\n        model: The name of the model (graph) to run.\n        messages: A list of OpenAI-compatible messages.\n        graph_registry: The registry containing the graph configurations.\n\n    Yields:\n        A tuple containing the content chunk and token usage metrics.\n    \"\"\"\n    logger.info(f\"Starting streaming LangGraph completion for model '{model}'\")\n\n    try:\n        graph_config = graph_registry.get_graph(model)\n        graph = await graph_config.resolve_graph()\n        streamable_node_names = graph_config.streamable_node_names\n    except ValueError as e:\n        logger.error(f\"Error getting graph for model '{model}': {e}\")\n        raise e\n\n    lc_messages = convert_to_lc_messages(messages)\n\n    inputs = {\"messages\": lc_messages}\n\n    callbacks = graph_config.runtime_callbacks\n\n    if settings.ENABLE_LANGFUSE is True:\n        if callbacks is None:\n            callbacks = []\n\n        callbacks.append(langfuse_handler)\n\n    runnable_config = RunnableConfig(callbacks=[callbacks]) if callbacks else None\n\n    async for event in graph.astream_events(\n        inputs, config=runnable_config, version=\"v2\"\n    ):\n        event_kind = event[\"event\"]\n        langgraph_node = event[\"metadata\"].get(\"langgraph_node\", None)\n\n        if event_kind == \"on_chat_model_stream\":\n            if langgraph_node not in streamable_node_names:\n                continue\n\n            ai_message_chunk: AIMessageChunk = event[\"data\"][\"chunk\"]\n            ai_message_content = ai_message_chunk.content\n            if ai_message_content:\n                yield f\"{ai_message_content}\", {\"tokens\": 1}\n</code></pre>"},{"location":"reference/#langgraph_openai_serve.graph.simple_graph","title":"simple_graph","text":"<p>Simple LangGraph agent implementation.</p> <p>This module defines a simple LangGraph agent that interfaces directly with an LLM model. It creates a straightforward workflow where a single node generates responses to user messages.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from langgraph_openai.utils.simple_graph import app\n&gt;&gt;&gt; result = await app.ainvoke({\"messages\": messages})\n&gt;&gt;&gt; print(result[\"messages\"][-1].content)\n</code></pre> <p>The module contains the following components: - <code>AgentState</code> - Pydantic BaseModel defining the state schema for the graph. - <code>generate(state)</code> - Function that processes messages and generates responses. - <code>workflow</code> - The StateGraph instance defining the workflow. - <code>app</code> - The compiled workflow application ready for invocation.</p>"},{"location":"reference/#langgraph_openai_serve.graph.simple_graph.AgentState","title":"AgentState","text":"<p>               Bases: <code>BaseModel</code></p> <p>Type definition for the agent state.</p> <p>This BaseModel defines the structure of the state that flows through the graph. It uses the add_messages annotation to properly handle message accumulation.</p> <p>Attributes:</p> Name Type Description <code>messages</code> <code>Annotated[Sequence[BaseMessage], add_messages]</code> <p>A sequence of BaseMessage objects annotated with add_messages.</p>"},{"location":"reference/#langgraph_openai_serve.graph.simple_graph.SimpleConfigSchema","title":"SimpleConfigSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configurable fields that are taken from the user</p>"},{"location":"reference/#langgraph_openai_serve.graph.simple_graph.generate","title":"generate  <code>async</code>","text":"<pre><code>generate(state, config)\n</code></pre> <p>Generate a response to the latest message in the state.</p> <p>This function extracts the latest message, creates a prompt with it, runs it through an LLM, and returns the response as an AIMessage.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>AgentState</code> <p>The current state containing the message history.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dict with a messages key containing the AI's response.</p> Source code in <code>src/langgraph_openai_serve/graph/simple_graph.py</code> <pre><code>async def generate(state: AgentState, config: SimpleConfigSchema) -&gt; dict:\n    \"\"\"Generate a response to the latest message in the state.\n\n    This function extracts the latest message, creates a prompt with it,\n    runs it through an LLM, and returns the response as an AIMessage.\n\n    Args:\n        state: The current state containing the message history.\n\n    Returns:\n        A dict with a messages key containing the AI's response.\n    \"\"\"\n    model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7, streaming=True)\n\n    system_message = (\n        \"system\",\n        \"You are a helpful assistant called Langgraph Openai Serve. Chat with the user with friendly tone\",\n    )\n\n    if config[\"configurable\"][\"use_history\"] is False:\n        question = state.messages[-1].content\n\n        prompt = ChatPromptTemplate.from_messages(\n            [system_message, (\"human\", \"{question}\")]\n        )\n\n        chain = prompt | model | StrOutputParser()\n        response = await chain.ainvoke({\"question\": question})\n    else:\n        messages = state.messages\n        prompt = ChatPromptTemplate.from_messages([system_message, *messages])\n        chain = prompt | model | StrOutputParser()\n        response = await chain.ainvoke({})\n\n    return {\n        \"messages\": [AIMessage(content=response)],\n    }\n</code></pre>"},{"location":"reference/#langgraph_openai_serve.openai_server","title":"openai_server","text":"<p>LangGraph OpenAI API Serve.</p> <p>This module provides a server class that connects LangGraph instances to an OpenAI-compatible API. It allows users to register their LangGraph instances and expose them through a FastAPI application.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from langgraph_openai_serve import LangchainOpenaiApiServe\n&gt;&gt;&gt; from fastapi import FastAPI\n&gt;&gt;&gt; from your_graphs import simple_graph_1, simple_graph_2\n&gt;&gt;&gt;\n&gt;&gt;&gt; app = FastAPI(title=\"LangGraph OpenAI API\")\n&gt;&gt;&gt; graph_serve = LangchainOpenaiApiServe(\n...     app=app,\n...     graphs={\n...         \"simple_graph_1\": simple_graph_1,\n...         \"simple_graph_2\": simple_graph_2\n...     }\n... )\n&gt;&gt;&gt; graph_serve.bind_openai_chat_completion(prefix=\"/v1\")\n</code></pre>"},{"location":"reference/#langgraph_openai_serve.openai_server.LangchainOpenaiApiServe","title":"LangchainOpenaiApiServe","text":"<pre><code>LangchainOpenaiApiServe(app=None, graphs=None, configure_cors=False)\n</code></pre> <p>Server class to connect LangGraph instances with an OpenAI-compatible API.</p> <p>This class serves as a bridge between LangGraph instances and an OpenAI-compatible API. It allows users to register their LangGraph instances and expose them through a FastAPI application.</p> <p>Attributes:</p> Name Type Description <code>app</code> <p>The FastAPI application to attach routers to.</p> <code>graphs</code> <p>A GraphRegistry instance containing the graphs to serve.</p> <p>Initialize the server with a FastAPI app (optional) and a GraphRegistry instance (optional).</p> <p>Parameters:</p> Name Type Description Default <code>app</code> <code>FastAPI | None</code> <p>The FastAPI application to attach routers to. If None, a new FastAPI app will be created.</p> <code>None</code> <code>graphs</code> <code>GraphRegistry | None</code> <p>A GraphRegistry instance containing the graphs to serve.     If None, a default simple graph will be used.</p> <code>None</code> <code>configure_cors</code> <code>bool</code> <p>Optional; Whether to configure CORS for the FastAPI application.</p> <code>False</code> Source code in <code>src/langgraph_openai_serve/openai_server.py</code> <pre><code>def __init__(\n    self,\n    app: FastAPI | None = None,\n    graphs: GraphRegistry | None = None,\n    configure_cors: bool = False,\n):\n    \"\"\"Initialize the server with a FastAPI app (optional) and a GraphRegistry instance (optional).\n\n    Args:\n        app: The FastAPI application to attach routers to. If None, a new FastAPI app will be created.\n        graphs: A GraphRegistry instance containing the graphs to serve.\n                If None, a default simple graph will be used.\n        configure_cors: Optional; Whether to configure CORS for the FastAPI application.\n    \"\"\"\n    self.app = app\n\n    if app is None:\n        app = FastAPI(\n            title=\"LangGraph OpenAI Compatible API\",\n            description=\"An OpenAI-compatible API for LangGraph\",\n            version=\"0.0.1\",\n        )\n    self.app = app\n\n    if graphs is None:\n        logger.info(\"Graphs not provided, using default simple graph\")\n        default_graph_config = GraphConfig(graph=simple_graph)\n        self.graph_registry = GraphRegistry(\n            registry={\"simple-graph\": default_graph_config}\n        )\n    elif isinstance(graphs, GraphRegistry):\n        logger.info(\"Using provided GraphRegistry instance\")\n        self.graph_registry = graphs\n    else:\n        raise TypeError(\n            \"Invalid type for graphs parameter. Expected GraphRegistry or None.\"\n        )\n\n    # Attach the registry to the app's state for dependency injection\n    self.app.state.graph_registry = self.graph_registry\n\n    # Configure CORS if requested\n    if configure_cors:\n        self._configure_cors()\n\n    logger.info(\n        f\"Initialized LangchainOpenaiApiServe with {len(self.graph_registry.registry)} graphs\"\n    )\n    logger.info(\n        f\"Available graphs: {', '.join(self.graph_registry.get_graph_names())}\"\n    )\n</code></pre>"},{"location":"reference/#langgraph_openai_serve.openai_server.LangchainOpenaiApiServe.bind_openai_chat_completion","title":"bind_openai_chat_completion","text":"<pre><code>bind_openai_chat_completion(prefix='/v1')\n</code></pre> <p>Bind OpenAI-compatible chat completion endpoints to the FastAPI app.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>Optional; The URL prefix for the OpenAI-compatible endpoints. Defaults to \"/v1\".</p> <code>'/v1'</code> Source code in <code>src/langgraph_openai_serve/openai_server.py</code> <pre><code>def bind_openai_chat_completion(self, prefix: str = \"/v1\"):\n    \"\"\"Bind OpenAI-compatible chat completion endpoints to the FastAPI app.\n\n    Args:\n        prefix: Optional; The URL prefix for the OpenAI-compatible endpoints. Defaults to \"/v1\".\n    \"\"\"\n    self.app.include_router(chat_views.router, prefix=prefix)\n    self.app.include_router(health_views.router, prefix=prefix)\n    self.app.include_router(models_views.router, prefix=prefix)\n\n    logger.info(f\"Bound OpenAI chat completion endpoints with prefix: {prefix}\")\n\n    return self\n</code></pre>"},{"location":"reference/#langgraph_openai_serve.schemas","title":"schemas","text":"<p>Models package for the LangGraph OpenAI compatible API.</p>"},{"location":"reference/#langgraph_openai_serve.utils","title":"utils","text":"<p>Utility functions.</p>"},{"location":"reference/#langgraph_openai_serve.utils.message","title":"message","text":""},{"location":"reference/#langgraph_openai_serve.utils.message.convert_to_lc_messages","title":"convert_to_lc_messages","text":"<pre><code>convert_to_lc_messages(messages)\n</code></pre> <p>Convert OpenAI messages to LangChain messages.</p> <p>This function converts a list of OpenAI-compatible message objects to their LangChain equivalents for use with LangGraph.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[ChatCompletionRequestMessage]</code> <p>A list of OpenAI chat completion request messages to convert.</p> required <p>Returns:</p> Type Description <code>list[BaseMessage]</code> <p>A list of LangChain message objects.</p> Source code in <code>src/langgraph_openai_serve/utils/message.py</code> <pre><code>def convert_to_lc_messages(\n    messages: list[ChatCompletionRequestMessage],\n) -&gt; list[BaseMessage]:\n    \"\"\"Convert OpenAI messages to LangChain messages.\n\n    This function converts a list of OpenAI-compatible message objects to their\n    LangChain equivalents for use with LangGraph.\n\n    Args:\n        messages: A list of OpenAI chat completion request messages to convert.\n\n    Returns:\n        A list of LangChain message objects.\n    \"\"\"\n\n    lc_messages = []\n    for m in messages:\n        if m.role == \"system\":\n            lc_messages.append(SystemMessage(content=m.content or \"\"))\n        elif m.role == \"user\":\n            lc_messages.append(HumanMessage(content=m.content or \"\"))\n        elif m.role == \"assistant\":\n            lc_messages.append(AIMessage(content=m.content or \"\"))\n    return lc_messages\n</code></pre>"},{"location":"explanation/","title":"Architecture Explanations","text":"<p>This section provides in-depth explanations of LangGraph OpenAI Serve's architecture, key concepts, and design decisions.</p>"},{"location":"explanation/#available-explanations","title":"Available Explanations","text":"<ul> <li>Architecture Overview - High-level overview of the system architecture</li> <li>Integration with LangGraph - How the library integrates with LangGraph</li> <li>OpenAI API Compatibility - How OpenAI API compatibility is achieved</li> </ul>"},{"location":"explanation/architecture/","title":"Architecture Overview","text":"<p>This document provides a high-level overview of the LangGraph OpenAI Serve architecture, explaining how the different components work together to provide an OpenAI-compatible API for LangGraph workflows.</p>"},{"location":"explanation/architecture/#system-architecture","title":"System Architecture","text":"<p>LangGraph OpenAI Serve consists of several key components:</p> <ol> <li>FastAPI Application: The web server that handles HTTP requests</li> <li>LangchainOpenaiApiServe: The core class that bridges LangGraph and the API</li> <li>Graph Registry: A registry that manages LangGraph instances</li> <li>API Routers: FastAPI routers for different API endpoints</li> <li>Schema Models: Pydantic models for data validation and serialization</li> </ol>"},{"location":"explanation/architecture/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                HTTP Clients                             \u2502\n\u2502    (OpenAI Python SDK, JavaScript SDK, curl, etc.)      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  FastAPI Application                    \u2502\n\u2502                                                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  Models Router  \u2502       \u2502  Chat Completions Router\u2502  \u2502\n\u2502  \u2502   /v1/models    \u2502       \u2502  /v1/chat/completions   \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502           \u2502                           \u2502                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502             LangchainOpenaiApiServe               \u2502  \u2502\n\u2502  \u2502                                                   \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502  \u2502\n\u2502  \u2502  \u2502              Graph Registry                 \u2502  \u2502  \u2502\n\u2502  \u2502  \u2502                                             \u2502  \u2502  \u2502\n\u2502  \u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502  \u2502  \u2502\n\u2502  \u2502  \u2502  \u2502 Graph 1   \u2502  \u2502 Graph 2   \u2502  \u2502 Graph N  \u2502 \u2502  \u2502  \u2502\n\u2502  \u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502  \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 LangGraph Workflows                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"explanation/architecture/#component-details","title":"Component Details","text":""},{"location":"explanation/architecture/#fastapi-application","title":"FastAPI Application","text":"<p>The FastAPI application serves as the web server that handles HTTP requests. It can be:</p> <ol> <li>Created automatically by LangchainOpenaiApiServe</li> <li>Provided by the user when they want to integrate LangGraph OpenAI Serve with an existing FastAPI application</li> </ol>"},{"location":"explanation/architecture/#langchainopenaiapiserve","title":"LangchainOpenaiApiServe","text":"<p>This is the core class that connects LangGraph workflows with the OpenAI-compatible API. Its responsibilities include:</p> <ul> <li>Managing the FastAPI application</li> <li>Registering and managing LangGraph instances</li> <li>Providing routers for different API endpoints</li> <li>Handling CORS configuration when needed</li> </ul>"},{"location":"explanation/architecture/#graph-registry","title":"Graph Registry","text":"<p>The Graph Registry maintains a mapping between model names and LangGraph instances. When a request comes in for a specific model, the registry looks up the corresponding LangGraph workflow to execute. The registry allows:</p> <ul> <li>Registering multiple graphs with different names</li> <li>Retrieving graphs by name</li> <li>Listing available graphs</li> </ul>"},{"location":"explanation/architecture/#api-routers","title":"API Routers","text":"<p>LangGraph OpenAI Serve provides several FastAPI routers:</p> <ol> <li>Models Router: Handles <code>/v1/models</code> endpoint to list available LangGraph workflows</li> <li>Chat Completions Router: Handles <code>/v1/chat/completions</code> endpoint for chat interactions</li> <li>Health Router: Provides a health check endpoint at <code>/health</code></li> </ol>"},{"location":"explanation/architecture/#schema-models","title":"Schema Models","text":"<p>Pydantic models are used for data validation and serialization. These include:</p> <ol> <li>Request Models: Define the structure of API requests</li> <li>Response Models: Define the structure of API responses</li> <li>OpenAI Compatible Models: Models that match OpenAI's API schema</li> </ol>"},{"location":"explanation/architecture/#request-flow","title":"Request Flow","text":"<p>When a client makes a request to the API, the following sequence of events occurs:</p> <ol> <li>Client Request: A client (like the OpenAI Python SDK) sends a request to the API</li> <li>FastAPI Router: The appropriate router handles the request based on the endpoint</li> <li>Request Validation: Pydantic models validate the request data</li> <li>Graph Selection: The system looks up the requested LangGraph workflow in the registry</li> <li>Graph Execution: The LangGraph workflow is executed with the provided messages</li> <li>Response Formatting: The result is formatted according to the OpenAI API schema</li> <li>Client Response: The response is sent back to the client</li> </ol>"},{"location":"explanation/architecture/#example-flow-for-chat-completion","title":"Example Flow for Chat Completion","text":"<pre><code>Client Request (POST /v1/chat/completions)\n    \u2502\n    \u25bc\nFastAPI Chat Router\n    \u2502\n    \u25bc\nRequest Validation (ChatCompletionRequest)\n    \u2502\n    \u25bc\nGraph Selection (get_graph_for_model)\n    \u2502\n    \u25bc\nMessage Conversion (convert_to_lc_messages)\n    \u2502\n    \u25bc\nGraph Execution (graph.ainvoke or graph.astream_events)\n    \u2502\n    \u25bc\nResponse Formatting\n    \u2502\n    \u25bc\nClient Response\n</code></pre>"},{"location":"explanation/architecture/#streaming-vs-non-streaming","title":"Streaming vs. Non-Streaming","text":"<p>LangGraph OpenAI Serve supports both streaming and non-streaming responses:</p>"},{"location":"explanation/architecture/#non-streaming-mode","title":"Non-Streaming Mode","text":"<p>In non-streaming mode: 1. The entire LangGraph workflow is executed 2. The final result is collected 3. A single response is returned to the client</p>"},{"location":"explanation/architecture/#streaming-mode","title":"Streaming Mode","text":"<p>In streaming mode: 1. The LangGraph workflow is executed with streaming enabled 2. Events from the workflow are captured in real-time 3. Each chunk of generated content is immediately sent to the client 4. The client receives and processes chunks as they arrive</p>"},{"location":"explanation/architecture/#integration-with-langgraph","title":"Integration with LangGraph","text":"<p>LangGraph OpenAI Serve integrates with LangGraph by:</p> <ol> <li>Accepting compiled LangGraph workflows (<code>graph.compile()</code>)</li> <li>Converting between OpenAI message formats and LangChain message formats</li> <li>Executing workflows with appropriate parameters (temperature, max_tokens, etc.)</li> <li>Handling both streaming and non-streaming execution modes</li> </ol>"},{"location":"explanation/architecture/#next-steps","title":"Next Steps","text":"<ul> <li>Read about integration with LangGraph for more details on how LangGraph workflows are executed</li> <li>Learn about OpenAI API compatibility to understand how the API matches OpenAI's interface</li> </ul>"},{"location":"explanation/langgraph-integration/","title":"Integration with LangGraph","text":"<p>This document explains how LangGraph OpenAI Serve integrates with LangGraph to execute workflows through an OpenAI-compatible API.</p>"},{"location":"explanation/langgraph-integration/#langgraph-overview","title":"LangGraph Overview","text":"<p>LangGraph is a library for building stateful, multi-actor applications with LLMs. It provides a way to create complex workflows where different components (or \"nodes\") can interact in a structured manner.</p> <p>Key concepts in LangGraph: - StateGraph: A graph where nodes modify a shared state - Nodes: Functions that process the state and return updates - Edges: Connections between nodes that define the flow of execution - Compiled Graphs: Executable workflows created by compiling a graph</p>"},{"location":"explanation/langgraph-integration/#how-langgraph-openai-serve-uses-langgraph","title":"How LangGraph OpenAI Serve Uses LangGraph","text":"<p>LangGraph OpenAI Serve acts as a bridge between the OpenAI API interface and your LangGraph workflows. Here's how it integrates with LangGraph:</p>"},{"location":"explanation/langgraph-integration/#1-graph-registration","title":"1. Graph Registration","text":"<p>LangGraph workflows are registered with a unique name that will be used as the \"model\" name in the OpenAI API:</p> <pre><code>from langgraph_openai_serve import LangchainOpenaiApiServe, GraphRegistry, GraphConfig\n\n# Assume simple_graph and advanced_graph are compiled LangGraph workflows\n# from your_graphs import simple_graph, advanced_graph\n\n# Create a GraphRegistry\ngraph_registry = GraphRegistry(\n    registry={\n        \"simple_graph\": GraphConfig(graph=simple_graph, streamable_node_names=[\"generate\"]),  # A compiled LangGraph workflow\n        \"advanced_graph\": GraphConfig(graph=advanced_graph, streamable_node_names=[\"generate\"]  # Another compiled LangGraph workflow\n    }\n)\n\ngraph_serve = LangchainOpenaiApiServe(\n    graphs=graph_registry\n)\n</code></pre>"},{"location":"explanation/langgraph-integration/#2-message-conversion","title":"2. Message Conversion","text":"<p>When a request comes in through the OpenAI API, the messages need to be converted from OpenAI format to LangChain format:</p> <pre><code>def convert_to_lc_messages(messages: list[ChatCompletionRequestMessage]) -&gt; list[BaseMessage]:\n    \"\"\"Convert OpenAI API messages to LangChain messages.\"\"\"\n    lc_messages = []\n    for message in messages:\n        if message.role == \"user\":\n            lc_messages.append(HumanMessage(content=message.content))\n        elif message.role == \"assistant\":\n            lc_messages.append(AIMessage(content=message.content))\n        elif message.role == \"system\":\n            lc_messages.append(SystemMessage(content=message.content))\n        # Handle more message types as needed\n    return lc_messages\n</code></pre>"},{"location":"explanation/langgraph-integration/#3-graph-execution","title":"3. Graph Execution","text":"<p>The LangGraph workflow is executed using the converted messages:</p>"},{"location":"explanation/langgraph-integration/#non-streaming-execution","title":"Non-Streaming Execution","text":"<p>For regular (non-streaming) requests, the graph is executed using <code>.ainvoke()</code>:</p> <pre><code># Convert OpenAI messages to LangChain messages\nlc_messages = convert_to_lc_messages(messages)\n\n# Run the graph with the messages\nresult = await graph.ainvoke({\"messages\": lc_messages})\n\n# Extract the response from the result\nresponse = result[\"messages\"][-1].content if result[\"messages\"] else \"\"\n</code></pre>"},{"location":"explanation/langgraph-integration/#streaming-execution","title":"Streaming Execution","text":"<p>For streaming requests, the graph is executed using <code>.astream_events()</code>:</p> <pre><code># Get streamable node names from the graph configuration\nstreamable_node_names = graph_config.streamable_node_names\ninputs = {\"messages\": lc_messages}\n\nasync for event in graph.astream_events(inputs, version=\"v2\"):\n    event_kind = event[\"event\"]\n    langgraph_node = event[\"metadata\"].get(\"langgraph_node\", None)\n\n    if event_kind == \"on_chat_model_stream\":\n        if langgraph_node not in streamable_node_names:\n            continue\n\n        ai_message_chunk: AIMessageChunk = event[\"data\"][\"chunk\"]\n        ai_message_content = ai_message_chunk.content\n        if ai_message_content:\n            yield f\"{ai_message_content}\", {\"tokens\": 1}\n</code></pre>"},{"location":"explanation/langgraph-integration/#4-response-formatting","title":"4. Response Formatting","text":"<p>After the graph is executed, the response is formatted according to the OpenAI API schema:</p> <pre><code>def format_completion_response(\n    model: str,\n    response_content: str,\n    token_usage: dict[str, int]\n) -&gt; ChatCompletion:\n    \"\"\"Format a response according to the OpenAI API schema.\"\"\"\n    return ChatCompletion(\n        id=f\"chatcmpl-{uuid.uuid4().hex}\",\n        created=int(time.time()),\n        model=model,\n        choices=[\n            ChatCompletionChoice(\n                index=0,\n                message=ChatCompletionResponseMessage(\n                    role=\"assistant\",\n                    content=response_content\n                ),\n                finish_reason=\"stop\"\n            )\n        ],\n        usage=ChatCompletionUsage(**token_usage)\n    )\n</code></pre>"},{"location":"explanation/langgraph-integration/#state-management","title":"State Management","text":"<p>LangGraph's core feature is state management, which allows for complex, multi-turn interactions. LangGraph OpenAI Serve preserves this capability by providing the messages as part of the state:</p> <pre><code># Define the state schema for our graph\nclass AgentState(BaseModel):\n    messages: Annotated[Sequence[BaseMessage], add_messages]\n\n# Define a node that processes messages\nasync def generate(state: AgentState):\n    \"\"\"Generate a response to the user's message.\"\"\"\n    messages = state.messages\n\n    # Process messages and generate a response\n    # ...\n\n    return {\n        \"messages\": [AIMessage(content=response)]\n    }\n</code></pre>"},{"location":"explanation/langgraph-integration/#default-simple-graph","title":"Default Simple Graph","text":"<p>LangGraph OpenAI Serve includes a default simple graph for users who want to get started quickly:</p> <pre><code># Define the state schema\nclass AgentState(BaseModel):\n    messages: Annotated[Sequence[BaseMessage], add_messages]\n\n# Define the generate function\nasync def generate(state: AgentState):\n    # Use the messages in the state to create a prompt for the LLM\n    messages = state.messages\n\n    # Create a simple LLM chain\n    llm = ChatOpenAI(temperature=0.7)\n    response = await llm.ainvoke(messages)\n\n    # Return the updated state with the new AI message\n    return {\"messages\": [response]}\n\n# Define the workflow graph\nworkflow = StateGraph(AgentState)\nworkflow.add_node(\"generate\", generate)\nworkflow.add_edge(\"generate\", END)\nworkflow.set_entry_point(\"generate\")\n\n# Compile the workflow\nsimple_graph = workflow.compile()\n</code></pre>"},{"location":"explanation/langgraph-integration/#supporting-advanced-langgraph-features","title":"Supporting Advanced LangGraph Features","text":"<p>LangGraph OpenAI Serve supports various advanced LangGraph features:</p>"},{"location":"explanation/langgraph-integration/#1-multi-node-graphs","title":"1. Multi-node Graphs","text":"<p>You can register complex graphs with multiple nodes:</p> <pre><code># Create a workflow with multiple nodes\nworkflow = StateGraph(AgentState)\nworkflow.add_node(\"parse\", parse_query)\nworkflow.add_node(\"search\", search_documents)\nworkflow.add_node(\"generate\", generate_response)\nworkflow.add_conditional_edges(\"parse\", ......)\nworkflow.add_edge(\"search\", \"generate\")\nworkflow.add_edge(\"generate\", END)\nworkflow.set_entry_point(\"parse\")\n\nadvanced_graph = workflow.compile()\n</code></pre>"},{"location":"explanation/langgraph-integration/#2-streaming-from-specific-nodes","title":"2. Streaming from Specific Nodes","text":"<p>LangGraph OpenAI Serve allows streaming content from specific nodes within your graph. This is configured using the <code>streamable_node_names</code> attribute in the <code>GraphConfig</code> when registering your graph. Only events originating from nodes listed in <code>streamable_node_names</code> will be streamed back to the client.</p> <pre><code>from langgraph_openai_serve import GraphConfig, GraphRegistry, LangchainOpenaiApiServe\n\n# Assume 'my_streaming_node_graph' has a node named 'streamer'\ngraph_config = GraphConfig(\n    graph=my_streaming_node_graph,\n    streamable_node_names=[\"streamer\"] # Specify which node(s) can stream\n)\n\nregistry = GraphRegistry(registry={\"streaming_model\": graph_config})\n\ngraph_serve = LangchainOpenaiApiServe(graphs=registry)\n# ... rest of the server setup\n</code></pre>"},{"location":"explanation/langgraph-integration/#3-toolfunction-calling","title":"3. Tool/Function Calling","text":"<p>OpenAI's function calling can be integrated with LangGraph's tool usage:</p> <pre><code># Define tools\ndef calculator(expression: str) -&gt; str:\n    \"\"\"Calculate the result of a mathematical expression.\"\"\"\n    try:\n        return str(eval(expression))\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n# Graph that uses tools\nasync def agent_with_tools(state: AgentState):\n    # Process messages and execute tools\n    # ...\n    pass\n</code></pre>"},{"location":"explanation/langgraph-integration/#considerations-for-langgraph-integration","title":"Considerations for LangGraph Integration","text":"<p>When integrating LangGraph workflows with the OpenAI API, consider the following:</p> <ol> <li>State Management: Design your state schema to handle conversation history effectively.</li> <li>Streaming Support: For better user experience, design your nodes to support streaming.</li> <li>Error Handling: Add robust error handling to your nodes to prevent crashes.</li> <li>Parameter Mapping: Consider how OpenAI API parameters like <code>temperature</code> and <code>max_tokens</code> should map to your graph.</li> </ol>"},{"location":"explanation/langgraph-integration/#next-steps","title":"Next Steps","text":"<ul> <li>Read about OpenAI API compatibility to understand how the API matches OpenAI's interface</li> <li>Learn how to create custom graphs for your specific use cases</li> <li>Check the API reference for detailed endpoint documentation</li> </ul>"},{"location":"explanation/openai-compatibility/","title":"OpenAI API Compatibility","text":"<p>This document explains how LangGraph OpenAI Serve achieves compatibility with the OpenAI API format, allowing clients to interact with your LangGraph workflows using standard OpenAI client libraries.</p>"},{"location":"explanation/openai-compatibility/#api-compatibility-layer","title":"API Compatibility Layer","text":"<p>LangGraph OpenAI Serve implements a subset of the OpenAI API, focusing on the most commonly used endpoints:</p> <ul> <li><code>/v1/models</code> - For listing available models (LangGraph instances)</li> <li><code>/v1/chat/completions</code> - For chat interactions</li> <li><code>/health</code> - For health checks</li> </ul> <p>The API is designed to be compatible with OpenAI client libraries in different languages while providing access to your custom LangGraph workflows.</p>"},{"location":"explanation/openai-compatibility/#schema-compatibility","title":"Schema Compatibility","text":"<p>The compatibility is achieved through carefully designed Pydantic models that mirror the OpenAI API schema:</p>"},{"location":"explanation/openai-compatibility/#request-schema","title":"Request Schema","text":"<pre><code>class ChatCompletionRequest(BaseModel):\n    model: str  # Maps to your LangGraph workflow name\n    messages: list[ChatCompletionRequestMessage]\n    temperature: float = 0.7\n    top_p: float = 1.0\n    max_tokens: int | None = None\n    stream: bool = False\n    tools: list[Tool] | None = None\n    stop: list[str] | None = None\n</code></pre>"},{"location":"explanation/openai-compatibility/#response-schema","title":"Response Schema","text":"<pre><code>class ChatCompletion(BaseModel):\n    id: str\n    object: str = \"chat.completion\"\n    created: int  # Unix timestamp\n    model: str\n    choices: list[ChatCompletionChoice]\n    usage: ChatCompletionUsage\n</code></pre>"},{"location":"explanation/openai-compatibility/#message-format-conversion","title":"Message Format Conversion","text":"<p>One of the key aspects of OpenAI compatibility is converting between OpenAI message formats and LangChain message formats:</p>"},{"location":"explanation/openai-compatibility/#openai-to-langchain-conversion","title":"OpenAI to LangChain Conversion","text":"<pre><code>def convert_to_lc_messages(messages: list[ChatCompletionRequestMessage]) -&gt; list[BaseMessage]:\n    \"\"\"Convert OpenAI API messages to LangChain messages.\"\"\"\n    lc_messages = []\n    for message in messages:\n        if message.role == \"user\":\n            lc_messages.append(HumanMessage(content=message.content))\n        elif message.role == \"assistant\":\n            lc_messages.append(AIMessage(content=message.content))\n        elif message.role == \"system\":\n            lc_messages.append(SystemMessage(content=message.content))\n        # Handle more message types as needed\n    return lc_messages\n</code></pre>"},{"location":"explanation/openai-compatibility/#langchain-to-openai-conversion","title":"LangChain to OpenAI Conversion","text":"<p>The reverse conversion happens when formatting responses:</p> <pre><code>def create_chat_completion(\n    model: str,\n    response_content: str,\n    token_usage: dict[str, int]\n) -&gt; ChatCompletion:\n    \"\"\"Create a ChatCompletion object from a response string.\"\"\"\n    return ChatCompletion(\n        id=f\"chatcmpl-{uuid.uuid4().hex}\",\n        created=int(time.time()),\n        model=model,\n        choices=[\n            ChatCompletionChoice(\n                index=0,\n                message=ChatCompletionResponseMessage(\n                    role=\"assistant\",\n                    content=response_content\n                ),\n                finish_reason=\"stop\"\n            )\n        ],\n        usage=ChatCompletionUsage(**token_usage)\n    )\n</code></pre>"},{"location":"explanation/openai-compatibility/#streaming-support","title":"Streaming Support","text":"<p>OpenAI's API supports streaming responses, which is particularly important for real-time interactions. LangGraph OpenAI Serve implements compatible streaming using Server-Sent Events (SSE):</p> <pre><code>async def create_chat_completion_stream(\n    request: Request,\n    body: ChatCompletionRequest,\n) -&gt; StreamingResponse:\n    \"\"\"Stream chat completion responses.\"\"\"\n    async def stream_generator():\n        unique_id = f\"chatcmpl-{uuid.uuid4().hex}\"\n        created = int(time.time())\n        model = body.model\n\n        # Initial response with role\n        yield f\"data: {json.dumps({\n            'id': unique_id,\n            'object': 'chat.completion.chunk',\n            'created': created,\n            'model': model,\n            'choices': [{\n                'index': 0,\n                'delta': {\n                    'role': 'assistant'\n                },\n                'finish_reason': None\n            }]\n        })}\\n\\n\"\n\n        # Generate content stream\n        async for content_chunk, metrics in run_langgraph_stream(\n            body.model,\n            body.messages,\n            body.temperature,\n            body.max_tokens,\n            body.tools if hasattr(body, 'tools') else None,\n        ):\n            yield f\"data: {json.dumps({\n                'id': unique_id,\n                'object': 'chat.completion.chunk',\n                'created': created,\n                'model': model,\n                'choices': [{\n                    'index': 0,\n                    'delta': {\n                        'content': content_chunk\n                    },\n                    'finish_reason': None\n                }]\n            })}\\n\\n\"\n\n        # Final message\n        yield f\"data: {json.dumps({\n            'id': unique_id,\n            'object': 'chat.completion.chunk',\n            'created': created,\n            'model': model,\n            'choices': [{\n                'index': 0,\n                'delta': {},\n                'finish_reason': 'stop'\n            }]\n        })}\\n\\n\"\n\n        # End of stream\n        yield \"data: [DONE]\\n\\n\"\n\n    return StreamingResponse(\n        stream_generator(),\n        media_type=\"text/event-stream\"\n    )\n</code></pre>"},{"location":"explanation/openai-compatibility/#functiontool-calling-support","title":"Function/Tool Calling Support","text":"<p>OpenAI's API supports tool calling, and LangGraph OpenAI Serve provides compatibility for this feature:</p> <pre><code>class FunctionCall(BaseModel):\n    name: str\n    arguments: str\n\nclass ToolCall(BaseModel):\n    id: str\n    type: str = \"function\"\n    function: FunctionCall\n\nclass Tool(BaseModel):\n    type: str = \"function\"\n    function: FunctionDefinition\n</code></pre> <p>When tools are provided in the request, they are passed to the LangGraph workflow, which can use them to generate function calls in the response.</p>"},{"location":"explanation/openai-compatibility/#differences-from-openai-api","title":"Differences from OpenAI API","text":"<p>While LangGraph OpenAI Serve aims for high compatibility, there are some differences to be aware of:</p> <ol> <li> <p>Model Selection: Instead of predefined OpenAI models, you specify your registered LangGraph workflow names.</p> </li> <li> <p>Feature Support: Not all OpenAI API features are supported. The focus is on chat completions with optional tool calling.</p> </li> <li> <p>Authentication: By default, authentication is not enforced, though you can add it as shown in the Authentication Guide.</p> </li> <li> <p>Token Counting: Token usage statistics are approximated rather than using OpenAI's tokenizer.</p> </li> </ol>"},{"location":"explanation/openai-compatibility/#client-compatibility","title":"Client Compatibility","text":"<p>The API is compatible with:</p> <ul> <li>OpenAI Python SDK</li> <li>OpenAI Node.js SDK</li> <li>Most other OpenAI-compatible clients</li> <li>Direct HTTP requests (e.g., with curl)</li> </ul>"},{"location":"explanation/openai-compatibility/#using-with-openai-client-libraries","title":"Using with OpenAI Client Libraries","text":"<p>Here's a simple example of using the OpenAI Python client with LangGraph OpenAI Serve:</p> <pre><code>from openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://localhost:8000/v1\",  # Your LangGraph OpenAI Serve URL\n    api_key=\"any-value\"  # API key is not verified by default\n)\n\nresponse = client.chat.completions.create(\n    model=\"my_custom_graph\",  # The name of your registered LangGraph workflow\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"explanation/openai-compatibility/#next-steps","title":"Next Steps","text":"<ul> <li>Read more about LangGraph integration to understand how LangGraph workflows are executed.</li> <li>Explore the API reference for detailed endpoint documentation.</li> </ul>"},{"location":"how-to-guides/","title":"How-To Guides","text":"<p>Welcome to the How-To Guides section for LangGraph OpenAI Serve. These guides provide practical instructions for specific tasks you might want to accomplish with the library.</p>"},{"location":"how-to-guides/#available-guides","title":"Available Guides","text":"<ul> <li>Docker Deployment - Learn how to deploy your LangGraph OpenAI Serve API using Docker</li> <li>Authentication - Add authentication to your API for improved security</li> </ul>"},{"location":"how-to-guides/authentication/","title":"Adding Authentication","text":"<p>This guide explains how to add authentication to your LangGraph OpenAI Serve API to enhance security.</p>"},{"location":"how-to-guides/authentication/#why-add-authentication","title":"Why Add Authentication?","text":"<p>By default, LangGraph OpenAI Serve doesn't enforce authentication. This is fine for local development or internal use, but for production environments, you should implement authentication to:</p> <ul> <li>Prevent unauthorized access to your API</li> <li>Track usage by different clients</li> <li>Apply rate limits or quotas to specific users</li> <li>Control access to different graph models</li> </ul>"},{"location":"how-to-guides/authentication/#simple-api-key-authentication","title":"Simple API Key Authentication","text":"<p>Here's how to implement a simple API key authentication system using FastAPI dependencies:</p>"},{"location":"how-to-guides/authentication/#1-create-an-authentication-module","title":"1. Create an Authentication Module","text":"<p>Create a file named <code>auth.py</code> with the following content:</p> <pre><code>from fastapi import Depends, HTTPException, Security\nfrom fastapi.security import APIKeyHeader\nfrom starlette.status import HTTP_403_FORBIDDEN\n\n# Define API key header\napi_key_header = APIKeyHeader(name=\"X-API-Key\", auto_error=False)\n\n# In a real application, store API keys in a database or environment variables\nAPI_KEYS = {\n    \"valid_user_1\": \"sk-valid-key-1\",\n    \"valid_user_2\": \"sk-valid-key-2\",\n}\n\nasync def get_api_key(api_key: str = Security(api_key_header)):\n    if not api_key:\n        raise HTTPException(\n            status_code=HTTP_403_FORBIDDEN,\n            detail=\"API key is missing\"\n        )\n\n    # Check if API key is valid\n    for user, key in API_KEYS.items():\n        if api_key == key:\n            # Optionally return user info for logging\n            return {\"user\": user, \"key\": api_key}\n\n    raise HTTPException(\n        status_code=HTTP_403_FORBIDDEN,\n        detail=\"Invalid API key\"\n    )\n</code></pre>"},{"location":"how-to-guides/authentication/#2-integrate-authentication-with-fastapi-app","title":"2. Integrate Authentication with FastAPI App","text":"<p>Now update your application to use this authentication:</p> <pre><code>from fastapi import FastAPI, Depends\nfrom langgraph_openai_serve import LangchainOpenaiApiServe\nfrom auth import get_api_key\n\n# Create a FastAPI app\napp = FastAPI(\n    title=\"Secure LangGraph API\",\n    description=\"LangGraph API with API key authentication\",\n)\n\n# Initialize the LangGraph OpenAI Serve\ngraph_serve = LangchainOpenaiApiServe(\n    app=app,\n    graphs={\n        \"my_graph\": my_graph,\n    },\n)\n\n# Bind the OpenAI-compatible endpoints with authentication\nchat_router = graph_serve.get_chat_router()\n\n# Add authentication dependency to the chat router\nfor route in chat_router.routes:\n    route.dependencies.append(Depends(get_api_key))\n\n# Include the router with authentication\napp.include_router(chat_router, prefix=\"/v1\", tags=[\"openai\"])\n\n# Add similar authentication to models router\nmodels_router = graph_serve.get_models_router()\nfor route in models_router.routes:\n    route.dependencies.append(Depends(get_api_key))\napp.include_router(models_router, prefix=\"/v1\", tags=[\"openai\"])\n</code></pre>"},{"location":"how-to-guides/authentication/#3-using-the-api-with-authentication","title":"3. Using the API with Authentication","text":"<p>Once authentication is enabled, clients need to provide the API key:</p> <pre><code>from openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://localhost:8000/v1\",\n    api_key=\"sk-valid-key-1\"  # A valid API key is now required\n)\n\nresponse = client.chat.completions.create(\n    model=\"my_graph\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello, how can you help me today?\"}\n    ]\n)\n</code></pre>"},{"location":"how-to-guides/authentication/#advanced-authentication-options","title":"Advanced Authentication Options","text":"<p>For more robust authentication, consider these approaches:</p>"},{"location":"how-to-guides/authentication/#oauth2-with-jwt","title":"OAuth2 with JWT","text":"<p>For more sophisticated applications, you might want to implement OAuth2 with JWT:</p> <pre><code>from datetime import datetime, timedelta\nfrom typing import Union\n\nfrom fastapi import Depends, FastAPI, HTTPException, status\nfrom fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm\nfrom jose import JWTError, jwt\nfrom passlib.context import CryptContext\nfrom pydantic import BaseModel\n\n# Secret key for JWT\nSECRET_KEY = \"your-secret-key\"\nALGORITHM = \"HS256\"\nACCESS_TOKEN_EXPIRE_MINUTES = 30\n\n# Password hashing\npwd_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\noauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"token\")\n\n# User database (in a real app, this would be a database)\nfake_users_db = {\n    \"user@example.com\": {\n        \"username\": \"user@example.com\",\n        \"hashed_password\": pwd_context.hash(\"userpassword\"),\n        \"disabled\": False,\n    }\n}\n\n# Token and user models\nclass Token(BaseModel):\n    access_token: str\n    token_type: str\n\nclass User(BaseModel):\n    username: str\n    disabled: Union[bool, None] = None\n\n# Authentication functions\ndef verify_password(plain_password, hashed_password):\n    return pwd_context.verify(plain_password, hashed_password)\n\ndef get_user(db, username: str):\n    if username in db:\n        user_dict = db[username]\n        return User(**user_dict)\n\ndef authenticate_user(fake_db, username: str, password: str):\n    user = get_user(fake_db, username)\n    if not user:\n        return False\n    if not verify_password(password, fake_db[username][\"hashed_password\"]):\n        return False\n    return user\n\ndef create_access_token(data: dict, expires_delta: timedelta = None):\n    to_encode = data.copy()\n    expire = datetime.utcnow() + (expires_delta or timedelta(minutes=15))\n    to_encode.update({\"exp\": expire})\n    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)\n    return encoded_jwt\n\nasync def get_current_user(token: str = Depends(oauth2_scheme)):\n    credentials_exception = HTTPException(\n        status_code=status.HTTP_401_UNAUTHORIZED,\n        detail=\"Invalid authentication credentials\",\n        headers={\"WWW-Authenticate\": \"Bearer\"},\n    )\n    try:\n        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])\n        username: str = payload.get(\"sub\")\n        if username is None:\n            raise credentials_exception\n    except JWTError:\n        raise credentials_exception\n    user = get_user(fake_users_db, username=username)\n    if user is None:\n        raise credentials_exception\n    return user\n\nasync def get_current_active_user(current_user: User = Depends(get_current_user)):\n    if current_user.disabled:\n        raise HTTPException(status_code=400, detail=\"Inactive user\")\n    return current_user\n\n# App setup with authentication\napp = FastAPI()\n\n@app.post(\"/token\", response_model=Token)\nasync def login_for_access_token(form_data: OAuth2PasswordRequestForm = Depends()):\n    user = authenticate_user(fake_users_db, form_data.username, form_data.password)\n    if not user:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Incorrect username or password\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n    access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)\n    access_token = create_access_token(\n        data={\"sub\": user.username}, expires_delta=access_token_expires\n    )\n    return {\"access_token\": access_token, \"token_type\": \"bearer\"}\n</code></pre>"},{"location":"how-to-guides/authentication/#using-authentication-middleware","title":"Using Authentication Middleware","text":"<p>For a more global approach, you can use middleware:</p> <pre><code>from fastapi import FastAPI, Request, HTTPException\nfrom fastapi.middleware.base import BaseHTTPMiddleware\nfrom starlette.status import HTTP_403_FORBIDDEN\n\nclass APIKeyMiddleware(BaseHTTPMiddleware):\n    async def dispatch(self, request: Request, call_next):\n        # Exclude certain paths from authentication (like health checks)\n        if request.url.path == \"/health\":\n            return await call_next(request)\n\n        # Check for API key in header\n        api_key = request.headers.get(\"X-API-Key\")\n        if not api_key or api_key not in [\"sk-valid-key-1\", \"sk-valid-key-2\"]:\n            return HTTPException(\n                status_code=HTTP_403_FORBIDDEN,\n                detail=\"Invalid or missing API key\",\n            )\n\n        # Continue processing the request\n        return await call_next(request)\n\napp = FastAPI()\napp.add_middleware(APIKeyMiddleware)\n</code></pre>"},{"location":"how-to-guides/authentication/#best-practices-for-api-authentication","title":"Best Practices for API Authentication","text":"<ol> <li> <p>Use HTTPS: Always use HTTPS in production to encrypt API keys and tokens in transit.</p> </li> <li> <p>Secure Storage: Store API keys and user credentials securely (not in code).</p> </li> <li> <p>Key Rotation: Implement a system to rotate API keys periodically.</p> </li> <li> <p>Scoped Access: Consider implementing scopes to limit what different API keys can access.</p> </li> <li> <p>Rate Limiting: Implement rate limiting based on API keys to prevent abuse.</p> </li> <li> <p>Logging: Log authentication attempts for security auditing.</p> </li> <li> <p>Revocation: Have a system to revoke API keys if they are compromised.</p> </li> </ol>"},{"location":"how-to-guides/authentication/#next-steps","title":"Next Steps","text":"<ul> <li>See API reference for detailed endpoint documentation</li> </ul>"},{"location":"how-to-guides/docker/","title":"Docker Deployment","text":"<p>This guide explains how to deploy your LangGraph OpenAI Serve API using Docker for production environments.</p>"},{"location":"how-to-guides/docker/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed:</p> <ul> <li>Docker</li> <li>Docker Compose (optional, but recommended)</li> </ul>"},{"location":"how-to-guides/docker/#using-the-provided-docker-setup","title":"Using the Provided Docker Setup","text":"<p>The LangGraph OpenAI Serve project comes with a ready-to-use Docker configuration in the <code>docker</code> directory. Here's how to use it:</p>"},{"location":"how-to-guides/docker/#building-and-running-the-docker-container","title":"Building and Running the Docker Container","text":"<p>You can use Docker Compose to build and run the container:</p> <pre><code># Start the server\ndocker compose up -d langgraph-openai-serve-dev\n</code></pre> <p>If you want to use the project with open-webui, a compatible UI for interacting with OpenAI-compatible APIs:</p> <pre><code># For a complete example with open-webui\ndocker compose up -d open-webui\n</code></pre>"},{"location":"how-to-guides/docker/#accessing-the-api","title":"Accessing the API","text":"<p>Once the container is running, you can access the API at:</p> <ul> <li>API: http://localhost:8000/v1</li> <li>OpenWebUI (if using): http://localhost:3000</li> </ul>"},{"location":"how-to-guides/docker/#creating-a-custom-docker-deployment","title":"Creating a Custom Docker Deployment","text":"<p>If you need to create a custom Docker deployment for your specific LangGraph OpenAI Serve application, follow these steps:</p>"},{"location":"how-to-guides/docker/#1-create-a-dockerfile","title":"1. Create a Dockerfile","text":"<p>Create a <code>Dockerfile</code> with the following content:</p> <pre><code>FROM python:3.11-slim\n\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\\n    gcc \\\n    &amp;&amp; apt-get clean \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Copy requirements and install dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\n# Expose the port\nEXPOSE 8000\n\n# Run the application\nCMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre>"},{"location":"how-to-guides/docker/#2-create-a-requirementstxt-file","title":"2. Create a requirements.txt File","text":"<p>Create a <code>requirements.txt</code> file with your dependencies:</p> <pre><code>langgraph-openai-serve\nuvicorn\n</code></pre> <p>Add any additional dependencies your application needs.</p>"},{"location":"how-to-guides/docker/#3-create-your-application","title":"3. Create Your Application","text":"<p>Create an <code>app.py</code> file with your custom LangGraph OpenAI Serve configuration:</p> <pre><code>from fastapi import FastAPI\nfrom langgraph_openai_serve import LangchainOpenaiApiServe, GraphRegistry, GraphConfig\n\n# Import your custom graphs\nfrom my_graphs import graph1, graph2\n\n# Create a FastAPI app\napp = FastAPI(\n    title=\"My LangGraph API\",\n    description=\"Custom LangGraph API with OpenAI compatibility\",\n)\n\n# Create a GraphRegistry\ngraph_registry = GraphRegistry(\n    registry={\n        \"graph1\": GraphConfig(graph=graph1, streamable_node_names=[\"generate\"]),\n        \"graph2\": GraphConfig(graph=graph2, streamable_node_names=[\"generate\"]),\n    }\n)\n\n# Initialize the LangGraph OpenAI Serve\ngraph_serve = LangchainOpenaiApiServe(\n    app=app,\n    graphs=graph_registry,\n    configure_cors=True,\n)\n\n# Bind the OpenAI-compatible endpoints\ngraph_serve.bind_openai_chat_completion(prefix=\"/v1\")\n</code></pre>"},{"location":"how-to-guides/docker/#4-create-a-docker-compose-file","title":"4. Create a Docker Compose File","text":"<p>For easier deployment, create a <code>docker-compose.yml</code> file:</p> <pre><code>version: '3'\n\nservices:\n  langgraph-api:\n    build: .\n    ports:\n      - \"8000:8000\"\n    environment:\n      - PYTHONUNBUFFERED=1\n    volumes:\n      - ./data:/app/data\n    restart: unless-stopped\n</code></pre>"},{"location":"how-to-guides/docker/#5-build-and-run","title":"5. Build and Run","text":"<p>Build and run your Docker container:</p> <pre><code>docker-compose up -d\n</code></pre>"},{"location":"how-to-guides/docker/#environment-variables","title":"Environment Variables","text":"<p>You can configure your deployment using environment variables:</p> Variable Description Default <code>PORT</code> The port to run the server on <code>8000</code> <code>HOST</code> The host to bind to <code>0.0.0.0</code> <code>LOG_LEVEL</code> Logging level <code>info</code> <p>You can pass these variables in your Docker Compose file or directly to Docker.</p>"},{"location":"how-to-guides/docker/#production-best-practices","title":"Production Best Practices","text":"<p>When deploying to production, consider the following best practices:</p> <ol> <li> <p>Use a Production ASGI Server: While Uvicorn is good for development, consider using Gunicorn with Uvicorn workers for production.</p> </li> <li> <p>Implement Authentication: Add proper authentication to protect your API.</p> </li> <li> <p>Set Up HTTPS: Use a reverse proxy like Nginx to handle HTTPS.</p> </li> <li> <p>Resource Constraints: Set appropriate memory and CPU limits for your Docker container.</p> </li> <li> <p>Monitoring: Implement monitoring for your service using tools like Prometheus and Grafana.</p> </li> <li> <p>Logging: Configure proper logging to capture errors and performance metrics.</p> </li> </ol>"},{"location":"how-to-guides/docker/#example-production-docker-compose","title":"Example Production Docker Compose","text":"<pre><code>version: '3'\n\nservices:\n  langgraph-api:\n    build: .\n    ports:\n      - \"8000:8000\"\n    environment:\n      - PYTHONUNBUFFERED=1\n      - LOG_LEVEL=warning\n    deploy:\n      resources:\n        limits:\n          cpus: '1'\n          memory: 1G\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 20s\n\n  nginx:\n    image: nginx:latest\n    ports:\n      - \"443:443\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf:ro\n      - ./ssl:/etc/ssl\n    depends_on:\n      - langgraph-api\n    restart: unless-stopped\n</code></pre>"},{"location":"how-to-guides/docker/#next-steps","title":"Next Steps","text":"<p>After deploying your API with Docker, you might want to:</p> <ul> <li>Implement authentication for your API</li> </ul>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Welcome to the LangGraph OpenAI Serve tutorials! These step-by-step guides will help you get started with using the library and explore its capabilities.</p>"},{"location":"tutorials/#available-tutorials","title":"Available Tutorials","text":"<ul> <li>Getting Started - Learn how to set up and run your first LangGraph OpenAI server</li> <li>Creating Custom Graphs - Learn how to create and register custom LangGraph instances</li> <li>Connecting with OpenAI Clients - Learn how to connect to your server using the OpenAI client libraries</li> </ul>"},{"location":"tutorials/custom-graphs/","title":"Creating Custom LangGraph Workflows","text":"<p>This tutorial explains how to create custom LangGraph workflows and expose them via the OpenAI-compatible API.</p>"},{"location":"tutorials/custom-graphs/#creating-a-basic-langgraph-workflow","title":"Creating a Basic LangGraph Workflow","text":"<p>LangGraph allows you to create complex workflow graphs for orchestrating LLM calls and other operations. Let's create a simple example:</p> <pre><code>from typing import Annotated, Sequence\nfrom pydantic import BaseModel\nfrom langchain_core.messages import AIMessage, BaseMessage\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import END, StateGraph\nfrom langgraph.graph.message import add_messages\n\n# Define the state schema for our graph\nclass AgentState(BaseModel):\n    messages: Annotated[Sequence[BaseMessage], add_messages]\n\n# Define the node function that processes messages\nasync def generate(state: AgentState):\n    \"\"\"Generate a response to the user's message.\"\"\"\n    # Use the messages in the state to create a prompt for the LLM\n    messages = state.messages\n\n    # Create a simple LLM chain\n    llm = ChatOpenAI(temperature=0.7)\n    prompt = ChatPromptTemplate.from_messages([\n        (\"system\", \"You are a helpful assistant.\"),\n        (\"user\", \"{input}\"),\n    ])\n    chain = prompt | llm | StrOutputParser()\n\n    # Get the last message from the user\n    last_message = messages[-1]\n    response = await chain.ainvoke({\"input\": last_message.content})\n\n    # Return the updated state with the new AI message\n    return {\n        \"messages\": [AIMessage(content=response)]\n    }\n\n# Define the workflow graph\nworkflow = StateGraph(AgentState)\nworkflow.add_node(\"generate\", generate)\nworkflow.add_edge(\"generate\", END)\nworkflow.set_entry_point(\"generate\")\n\n# Compile the workflow for execution\ncustom_graph = workflow.compile()\n</code></pre>"},{"location":"tutorials/custom-graphs/#exposing-your-custom-graph","title":"Exposing Your Custom Graph","text":"<p>After creating your custom graph, you can expose it through the OpenAI-compatible API:</p> <pre><code>from langgraph_openai_serve import LangchainOpenaiApiServe, GraphRegistry, GraphConfig\n\n# Assume custom_graph is your compiled LangGraph instance\n# from my_custom_graph_module import custom_graph\n\n# Create a GraphRegistry\ngraph_registry = GraphRegistry(\n    registry={\n        \"my-custom-graph\": GraphConfig(graph=custom_graph, streamable_node_names=[\"generate\"]),\n    }\n)\n\n# Create a server instance with your custom graph\ngraph_serve = LangchainOpenaiApiServe(\n    graphs=graph_registry,\n)\n\n# Bind the OpenAI-compatible endpoints\ngraph_serve.bind_openai_chat_completion(prefix=\"/v1\")\n\n# Run the app with uvicorn\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(graph_serve.app, host=\"0.0.0.0\", port=8000)\n</code></pre>"},{"location":"tutorials/custom-graphs/#creating-more-advanced-graphs","title":"Creating More Advanced Graphs","text":"<p>For more advanced use cases, you can create graphs with multiple nodes and complex logic:</p> <pre><code>from typing import Annotated, Sequence, TypedDict\nfrom pydantic import BaseModel, Field\nimport operator\nfrom langchain_core.messages import AIMessage, BaseMessage, HumanMessage\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import END, StateGraph\nfrom langgraph.graph.message import add_messages\n\n# Define a more complex state\nclass AgentState(BaseModel):\n    messages: Annotated[Sequence[BaseMessage], add_messages]\n    should_search: bool = False\n    search_results: str = \"\"\n\n# Define node functions\nasync def router(state: AgentState):\n    \"\"\"Route to the appropriate tool based on the user's query.\"\"\"\n    query = state.messages[-1].content.lower()\n\n    if \"search\" in query or \"find\" in query or \"look up\" in query:\n        return {\"should_search\": True}\n    else:\n        return {\"should_search\": False}\n\nasync def search_tool(state: AgentState):\n    \"\"\"Simulate a search operation.\"\"\"\n    query = state.messages[-1].content\n    # In a real implementation, you would call a search API here\n    search_results = f\"Found the following information about '{query}': This is simulated search data.\"\n    return {\"search_results\": search_results}\n\nasync def generate_response(state: AgentState):\n    \"\"\"Generate a response based on messages and any search results.\"\"\"\n    llm = ChatOpenAI(temperature=0.7)\n\n    messages = [\n        HumanMessage(content=\"You are a helpful assistant with search capabilities.\")\n    ]\n\n    # Add all the conversation history\n    messages.extend(state.messages)\n\n    # If we have search results, add them\n    if state.search_results:\n        messages.append(HumanMessage(content=f\"Search results: {state.search_results}\\nPlease use this information in your response.\"))\n\n    # Generate a response\n    ai_response = await llm.ainvoke(messages)\n\n    return {\"messages\": [AIMessage(content=ai_response.content)]}\n\n# Create the workflow\nworkflow = StateGraph(AgentState)\n\n# Add nodes\nworkflow.add_node(\"router\", router)\nworkflow.add_node(\"search\", search_tool)\nworkflow.add_node(\"generate\", generate_response)\n\n# Add conditional edges\nworkflow.add_conditional_edges(\n    \"router\",\n    {\n        True: \"search\",\n        False: \"generate\"\n    },\n    key=operator.itemgetter(\"should_search\")\n)\n\nworkflow.add_edge(\"search\", \"generate\")\nworkflow.add_edge(\"generate\", END)\n\nworkflow.set_entry_point(\"router\")\n\n# Compile the workflow\nadvanced_graph = workflow.compile()\n</code></pre>"},{"location":"tutorials/custom-graphs/#best-practices-for-graph-creation","title":"Best Practices for Graph Creation","text":"<p>When creating graphs for use with LangGraph OpenAI Serve, consider the following best practices:</p> <ol> <li>State Management: Design your state schema carefully to include all necessary information.</li> <li>Error Handling: Add error handling to your node functions to prevent crashes.</li> <li>Naming Conventions: Use clear, descriptive names for graphs and nodes.</li> <li>Streaming Support: For better user experience, design your graph to support streaming responses when possible. Configure which nodes should stream by setting the <code>streamable_node_names</code> list in the <code>GraphConfig</code> when registering your graph.</li> <li>Documentation: Document what each graph does to make it easier for API users to choose the right model.</li> </ol>"},{"location":"tutorials/custom-graphs/#next-steps","title":"Next Steps","text":"<p>Once you've created your custom graphs, you might want to:</p> <ul> <li>Connect with OpenAI clients to interact with your graphs</li> <li>Learn about deploying with Docker for production use</li> <li>Explore how to add authentication to your API</li> </ul>"},{"location":"tutorials/getting-started/","title":"Getting Started with LangGraph OpenAI Serve","text":"<p>This tutorial will guide you through the process of setting up and running your first LangGraph OpenAI compatible server.</p>"},{"location":"tutorials/getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have:</p> <ul> <li>Python 3.11 or higher installed</li> <li>Basic familiarity with LangGraph</li> <li>Basic understanding of FastAPI (optional)</li> </ul>"},{"location":"tutorials/getting-started/#installation","title":"Installation","text":"<p>First, install the <code>langgraph-openai-serve</code> package:</p> <pre><code># Using uv (recommended)\nuv add langgraph-openai-serve\n\n# Using pip\npip install langgraph-openai-serve\n</code></pre>"},{"location":"tutorials/getting-started/#basic-usage","title":"Basic Usage","text":"<p>Here's a simple example to help you get started. In this example, we'll create a basic server that exposes a simple LangGraph workflow through an OpenAI-compatible API.</p>"},{"location":"tutorials/getting-started/#1-create-a-python-file-for-your-server","title":"1. Create a Python file for your server","text":"<p>Create a new file called <code>server.py</code> with the following content:</p> <pre><code>from langgraph_openai_serve import LangchainOpenaiApiServe, GraphRegistry, GraphConfig\n\n# Import your LangGraph instances or use the default simple graph\n# that comes with the package\nfrom langgraph_openai_serve.graph.simple_graph import app as simple_graph\n\n# Create a GraphRegistry\ngraph_registry = GraphRegistry(\n    registry={\n        \"simple_graph\": GraphConfig(graph=simple_graph, streamable_node_names=[\"generate\"]),\n    }\n)\n\n# Create a server instance with your graph(s)\ngraph_serve = LangchainOpenaiApiServe(\n    graphs=graph_registry,\n    configure_cors=True,  # Enable CORS for browser clients\n)\n\n# Bind the OpenAI-compatible endpoints\ngraph_serve.bind_openai_chat_completion(prefix=\"/v1\")\n\n# Run the app with uvicorn\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(graph_serve.app, host=\"0.0.0.0\", port=8000)\n</code></pre>"},{"location":"tutorials/getting-started/#2-run-the-server","title":"2. Run the server","text":"<p>Start the server by running:</p> <pre><code>python server.py\n</code></pre> <p>Your server should now be running at http://localhost:8000</p>"},{"location":"tutorials/getting-started/#3-test-the-api","title":"3. Test the API","text":"<p>You can test the API using curl:</p> <pre><code>curl -X POST http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"simple_graph\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Hello, how can you help me today?\"}\n    ]\n  }'\n</code></pre> <p>Alternatively, you can use the OpenAI Python client:</p> <pre><code>from openai import OpenAI\n\n# Create a client pointing to your API\nclient = OpenAI(\n    base_url=\"http://localhost:8000/v1\",\n    api_key=\"any-value\"  # API key is not verified\n)\n\n# Use the graph by specifying its name as the model\nresponse = client.chat.completions.create(\n    model=\"simple_graph\",  # This maps to the graph name in your registry\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello, how can you help me today?\"}\n    ]\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"tutorials/getting-started/#using-with-your-own-fastapi-application","title":"Using with your own FastAPI application","text":"<p>If you already have a FastAPI application, you can integrate LangGraph OpenAI Serve with it:</p> <pre><code>from fastapi import FastAPI\nfrom langgraph_openai_serve import LangchainOpenaiApiServe, GraphRegistry, GraphConfig\nfrom langgraph_openai_serve.graph.simple_graph import app as simple_graph\n\n# Create a FastAPI app\napp = FastAPI(\n    title=\"My API with LangGraph\",\n    version=\"1.0\",\n    description=\"API that includes LangGraph capabilities\",\n)\n\n# Create a GraphRegistry\ngraph_registry = GraphRegistry(\n    registry={\n        \"simple_graph\": GraphConfig(graph=simple_graph, streamable_node_names=[\"generate\"]),\n    }\n)\n\n# Create the LangchainOpenaiApiServe instance\ngraph_serve = LangchainOpenaiApiServe(\n    app=app,  # Pass in your existing FastAPI app\n    graphs=graph_registry,\n)\n\n# Bind the OpenAI-compatible endpoints\ngraph_serve.bind_openai_chat_completion(prefix=\"/v1\")\n\n# Add your other routes as needed\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"Welcome to my API with LangGraph integration!\"}\n\n# Run the app with uvicorn\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n</code></pre>"},{"location":"tutorials/getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you have a basic understanding of how to use LangGraph OpenAI Serve, you might want to:</p> <ol> <li>Create custom graphs to expose through your API</li> <li>Learn more about connecting with OpenAI clients</li> <li>Explore Docker deployment for production use</li> </ol>"},{"location":"tutorials/makefile/","title":"Useful Makefile commands","text":"<pre><code># All available commands\nmake\nmake help\n\n# Run all tests\nmake -s test\n\n# Run specific tests\nmake -s test-one TEST_MARKER=&lt;TEST_MARKER&gt;\n\n# Remove unnecessary files such as build,test, cache\nmake -s clean\n\n# Run all pre-commit hooks\nmake -s pre-commit\n\n# Lint the project\nmake -s lint\n\n# Profile a file\nmake -s profile PROFILE_FILE_PATH=&lt;PATH_TO_FILE&gt;\n</code></pre>"},{"location":"tutorials/openai-clients/","title":"Connecting with OpenAI Clients","text":"<p>Once you have your LangGraph OpenAI Serve API running, you can connect to it using any OpenAI-compatible client. This tutorial shows how to interact with your API using various clients and libraries.</p>"},{"location":"tutorials/openai-clients/#python-openai-client","title":"Python OpenAI Client","text":"<p>The most common way to connect to your API is using the official OpenAI Python client:</p> <pre><code>from openai import OpenAI\n\n# Initialize client with your custom base URL\nclient = OpenAI(\n    base_url=\"http://localhost:8000/v1\",  # Replace with your API URL\n    api_key=\"any-value\"  # API key is not verified in the default setup\n)\n\n# Making a standard chat completion request\nresponse = client.chat.completions.create(\n    model=\"my-custom-graph\",  # Use the name of your registered graph\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What's the capital of France?\"}\n    ]\n)\n\n# Accessing the response\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"tutorials/openai-clients/#streaming-responses","title":"Streaming Responses","text":"<p>To use streaming with the OpenAI client:</p> <pre><code>from openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://localhost:8000/v1\",\n    api_key=\"any-value\"\n)\n\n# Create a streaming completion\nstream = client.chat.completions.create(\n    model=\"my-custom-graph\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Write a short poem about AI.\"}\n    ],\n    stream=True  # Enable streaming\n)\n\n# Process the streaming response\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\")\n</code></pre>"},{"location":"tutorials/openai-clients/#javascripttypescript-client","title":"JavaScript/TypeScript Client","text":"<p>For web applications, you can use the OpenAI JavaScript client:</p> <pre><code>import OpenAI from 'openai';\n\nconst openai = new OpenAI({\n  baseURL: 'http://localhost:8000/v1',\n  apiKey: 'any-value',\n  dangerouslyAllowBrowser: true // For client-side use\n});\n\nasync function getChatCompletion() {\n  const completion = await openai.chat.completions.create({\n    model: 'my-custom-graph',\n    messages: [\n      { role: 'system', content: 'You are a helpful assistant.' },\n      { role: 'user', content: 'What\\'s the capital of France?' }\n    ],\n  });\n\n  console.log(completion.choices[0].message.content);\n}\n\ngetChatCompletion();\n</code></pre>"},{"location":"tutorials/openai-clients/#streaming-with-javascript","title":"Streaming with JavaScript","text":"<pre><code>import OpenAI from 'openai';\n\nconst openai = new OpenAI({\n  baseURL: 'http://localhost:8000/v1',\n  apiKey: 'any-value',\n  dangerouslyAllowBrowser: true\n});\n\nasync function streamChatCompletion() {\n  const stream = await openai.chat.completions.create({\n    model: 'my-custom-graph',\n    messages: [\n      { role: 'system', content: 'You are a helpful assistant.' },\n      { role: 'user', content: 'Write a short poem about AI.' }\n    ],\n    stream: true,\n  });\n\n  let responseText = '';\n  for await (const chunk of stream) {\n    const content = chunk.choices[0]?.delta?.content || '';\n    responseText += content;\n    console.log(content); // Update UI with new content\n  }\n}\n\nstreamChatCompletion();\n</code></pre>"},{"location":"tutorials/openai-clients/#using-with-curl","title":"Using with curl","text":"<p>You can also use curl to interact with your API directly:</p> <pre><code>curl -X POST http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"my-custom-graph\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n  }'\n</code></pre>"},{"location":"tutorials/openai-clients/#using-with-other-openai-client-libraries","title":"Using with Other OpenAI Client Libraries","text":"<p>Most OpenAI client libraries in different programming languages will work with your LangGraph OpenAI Serve API, as long as they allow you to set a custom base URL. Here's a general pattern:</p> <ol> <li>Initialize the client with your custom base URL</li> <li>Set any API key (as it's not verified by default)</li> <li>Make API calls as you would with the official OpenAI API</li> </ol>"},{"location":"tutorials/openai-clients/#available-endpoints","title":"Available Endpoints","text":"<p>The following endpoints are available in your LangGraph OpenAI Serve API:</p> <ul> <li><code>GET /v1/models</code> - List available models (your registered graphs)</li> <li><code>POST /v1/chat/completions</code> - Create a chat completion</li> <li><code>GET /health</code> - Check the health status of the API</li> </ul>"},{"location":"tutorials/openai-clients/#best-practices","title":"Best Practices","text":"<ol> <li>Error Handling: Always implement proper error handling in your client code</li> <li>Timeouts: Set appropriate request timeouts, especially for complex graph workflows</li> <li>API Key: For production, consider implementing authentication and using proper API keys</li> <li>Model Selection: Make sure to use the correct model name (graph name) in your requests</li> <li>Streaming: For longer responses or better user experience, use streaming when possible</li> </ol>"},{"location":"tutorials/openai-clients/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about deploying with Docker</li> <li>Explore how to add authentication to your API</li> <li>See the API reference for detailed endpoint documentation</li> </ul>"}]}